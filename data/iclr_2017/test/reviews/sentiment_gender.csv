comment, title,  sentiment_score, acceptance, gender 
 Extended the paper with experiments on the word relationship dataset showing Doc2VecC generates better word embeddings in comparison to Word2Vec or Paragraph Vectors. ,Efficient Vector Representation for Documents through Corruption,0.4404,True,['female']
I'm using ,Efficient Vector Representation for Documents through Corruption,0.0,True,['female']
The introduced method for producing document representations is simple efficient and potentially quite useful. Though we could quibble a bit that the idea is just a combination of known techniques the reviews generally agree that the idea is interesting.  Pros: + interesting and simple algorithm + strong performance + efficient  Cons: + individual ideas are not so novel  This is a paper that will be well received at a poster presentation.,Efficient Vector Representation for Documents through Corruption,2.1687,True,['female']
Dear reviewers I added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods noticeably the dependency-tree RNNs introduced in [1] which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand it is much faster to train and test. As reported in the original paper training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.I also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). I would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. [1] Socher Richard et al. "Grounded compositional semantics for finding and describing images with sentences." Transactions of the Association for Computational Linguistics 2 (2014): 207-218.,Efficient Vector Representation for Documents through Corruption,1.3767,True,['female']
Dear reviewers Thank you for your feedback. The updated manuscript included skip-thought as another baseline method. We will test this idea on more datasets in particular the ones experimented in Skip-thought vectors in the submission. ,Efficient Vector Representation for Documents through Corruption,0.6249,True,['female']
This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new I believe that the combination in this fashion is. Further I appreciated the detailed analysis of model behaviour in section 3.The main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?While I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines I think this paper should be accepted: it's an interesting algorithm nicely composed and very efficient so it's reasonable to assume that other readers might have use for some of the ideas presented here.,Efficient Vector Representation for Documents through Corruption,0.3928000000000001,True,['female']
This paper presents a framework for creating document representations. The main idea is to represent a document as an average of its word embeddings with a data-dependent regularization that favors informative or rare words while forcing common words to be close to 0. Experiments on sentiment analysis and document classification show that the proposed method has the lowest error rates compared to baseline document embedding methods. While I like the motivation of finding the best way to encode a document into a vector the paper does not offer significant technical contributions.Most of the techniques are not new and the main selling point is the simplicity and speed of the proposed method. For this reason I would like to see good results for more than two tasks to be convinced that this is the best way to learn document representations.  For RNN-LM is the LM trained to minimize classification error or is it trained  as a language model? Did you use the final hidden state as the representation or the average of all hidden states?One of the most widely used method to represent documents now is to have a bidirectional LSTM and concatenate the final hidden states as the document representation. I think it would be useful to know how the proposed method compares to this approach for tasks such as document classification or sentiment analysis.,Efficient Vector Representation for Documents through Corruption,1.6601,True,['female']
Unsupervised document representations is an active area of research so it would be useful to benchmark against something more recent than doc2vec which was in ICML 2014. Skip-thought vectors in particular should really be included.,Efficient Vector Representation for Documents through Corruption,0.6808,True,['female']
This paper proposes learning document embeddings as a sum of the constituent word embeddings which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel the result is an efficient learning algorithm for document representation with good empirical performance.Joint training of word and document embeddings is not a new idea nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see e.g. '“The Sum of Its Parts”: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods this paper offers little on the novelty front.On the other hand it is very efficient at generation time requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover by construction the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model the performance on sentiment analysis and document classification is quite encouraging.Overall despite the lack of novelty the simplicity efficiency and performance of this model make it worthy of wider readership and study and I recommend acceptance.,Efficient Vector Representation for Documents through Corruption,2.9725,True,['female']
,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0,False,['male' 'male' 'male' 'male']
The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets consisting of a single filter bank with random weights work surprisingly well and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function which is a reasonable explanation. In the new supplementary material the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results which is a useful complement to the original experiments.The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution and measuring how well a method meets this challenge remains an open problem. Hence while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics) it is difficult to conclusively confirm that this is the case.The authors indicate that diversity could be measured in terms of entropy. This is reasonable but as they acknowledge in their answers to questions difficult to do in practice. Furthermore this would still not account for the other aspect of the problem namely visual quality. They also suggest to perform a psychophysical assessment which may be the only practical way of addressing this problem but deem that to be material for future work.Overall since evaluation of image generation is such an hard problem I think the paper still has sufficient strengths to warrant publication in ICLR. Still some form of psychophysical assessment would be useful to confirm the intuitions that at present can only be obtained by inspecting the figure sin the paper and in the supplementary material.,What does it take to generate natural textures?,2.8994999999999993,True,['male' 'male' 'male' 'male']
The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective this is unfortunately a common issue in this field. Overall this paper is a solid contribution with findings that would be interesting to the ICLR audience.,What does it take to generate natural textures?,1.3994,True,['male' 'male' 'male' 'male']
This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. This work is indeed interesting and insightful. However the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ,What does it take to generate natural textures?,1.3917,True,['male' 'male' 'male' 'male']
This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising and analysis is quite thorough. I do think the evaluation methods require more work but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which I think should be accepted.,What does it take to generate natural textures?,1.7104,True,['male' 'male' 'male' 'male']
The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets consisting of a single filter bank with random weights work surprisingly well and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function which is a reasonable explanation. In the new supplementary material the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results which is a useful complement to the original experiments.The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution and measuring how well a method meets this challenge remains an open problem. Hence while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics) it is difficult to conclusively confirm that this is the case.The authors indicate that diversity could be measured in terms of entropy. This is reasonable but as they acknowledge in their answers to questions difficult to do in practice. Furthermore this would still not account for the other aspect of the problem namely visual quality. They also suggest to perform a psychophysical assessment which may be the only practical way of addressing this problem but deem that to be material for future work.Overall since evaluation of image generation is such an hard problem I think the paper still has sufficient strengths to warrant publication in ICLR. Still some form of psychophysical assessment would be useful to confirm the intuitions that at present can only be obtained by inspecting the figure sin the paper and in the supplementary material.,What does it take to generate natural textures?,2.8994999999999993,True,['male' 'male' 'male' 'male']
The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. Comments:- It's not clear to me why this should be called a "statistician". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach etc. In general it felt like the paper could be more clear if it avoided coining new terms like "statistic network" and stuck to the more accurate "approximate posterior".- The experiments are nice and I appreciate the response to my question regarding "one shot generation". I still think that language needs to be clarified specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set compute the approximate posterior over the context vector then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: (a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? (b) I agree that the samples are of high-quality but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end one "proper" way of computing the "one shot generation" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.,Towards a Neural Statistician,3.8661000000000003,True,['male' 'male']
This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.,Towards a Neural Statistician,1.4314,True,['male' 'male']
This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a "double" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  Hierarchical modeling is an important and high impact problem and I think that it's under-explored in the Deep Learning literature.  Pros:  -The few-shot learning results look good but I'm not an expert in this area.    -The idea of using a "double" variational bound in a hierarchical generative model is well presented and seems widely applicable.  Questions:   -When training the statistic network are minibatches (i.e. subsets of the examples) used?    -If not does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches it won't be able to learn this characterization because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  Suggestions:   -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ,Towards a Neural Statistician,1.0815,True,['male' 'male']
Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to "learn to learn" by acquiring the ability to learn distributions from small numbers of examples.Overall this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant and seems to perform well. Compared to other recent papers on one-shot learning the proposed method is simpler and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician unless one means by that "someone who thinks up statistics". The experiments are well chosen and the few-shot learning results seem pretty solid given the simplicity of the method.The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense though; shouldn’t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) Will the authors release the code?,Towards a Neural Statistician,4.0916,True,['male' 'male']
The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach soft attention is first used to derive alignments between the tokens of the two texts then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well and offers a detailed empirical analysis of the comparison component of the model.This work is timely. Language understanding problems of this kind are a major open issue in NLP and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.) but it contributes in enough substantial ways that I'd strongly recommend acceptance.Detail: - The model at least as implemented for the problems with longer sequences (everything but SNLI) is not sensitive to word order. It is empirically competitive but this insensitivity places a strong upper bound on its performance. The paper does make this clear but it seems salient enough to warrant a brief mention in the introduction or discussion sections.- If I understand correctly your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).- Since the NTN risks overfitting because of its large number of parameters did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.,A Compare-Aggregate Model for Matching Text Sequences,2.5897000000000006,True,[None 'female']
This paper proposes a framework whereby to an attention mechanism relating one text segment to another piecewise an aggregation mechanism is added to yield an architecture matching words of one segment to another. Different vector comparison operations are explored in this framework. The reviewers were satisfied that this work is relevant timely clearly presented and that the empirical validation was sound. ,A Compare-Aggregate Model for Matching Text Sequences,0.6705,True,[None 'female']
This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences such as question answering and textual entailment.The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison where this paper compares several different methods for matching text sequences and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.   ,A Compare-Aggregate Model for Matching Text Sequences,0.5513000000000001,True,[None 'female']
This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines.The paper is well written overall.A few detailed comments:* page 4 line5: including a some -> including some* What's the benefit of the preprocessing and attention step? Can you provide the results without it?* Figure 2 is hard to read esp. when on printed hard copy. Please enhance the quality.,A Compare-Aggregate Model for Matching Text Sequences,0.8448,True,[None 'female']
The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach soft attention is first used to derive alignments between the tokens of the two texts then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well and offers a detailed empirical analysis of the comparison component of the model.This work is timely. Language understanding problems of this kind are a major open issue in NLP and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.) but it contributes in enough substantial ways that I'd strongly recommend acceptance.Detail: - The model at least as implemented for the problems with longer sequences (everything but SNLI) is not sensitive to word order. It is empirically competitive but this insensitivity places a strong upper bound on its performance. The paper does make this clear but it seems salient enough to warrant a brief mention in the introduction or discussion sections.- If I understand correctly your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).- Since the NTN risks overfitting because of its large number of parameters did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.,A Compare-Aggregate Model for Matching Text Sequences,2.5897000000000006,True,[None 'female']
Text matching models based on Attention mechanism make sense. There are also some matching models based on Matching Matrix.Attention mechanism also computes a matching matrix implicitly and the attention weights before softmax are the values of the matching matrix. I wonder which way is better Attention or Matching Matrix and Why?How do you think？I will appreciate it if you could compare these models in your future works.Reference of Matching Matrix Models:1. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. AAAI 2016.2. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. IJCAI 2016.3. Text Matching as Image Recognition. AAAI 2016.,A Compare-Aggregate Model for Matching Text Sequences,1.3214,True,[None 'female']
This paper poses an interesting idea: removing chaotic behavior or RNNs.While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box this paper does a good job on presenting why its method may work well.Although the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model) the reviewer is not fully convinced by the main claim of this paper the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper the LSTM shows a very sensitive behaviour even when a very small amount of noise is added to the input. However it still performs surprisingly well with this chaotic behaviour. Measuring the model complexity is a very difficult task therefore many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However I think the CFN may have much more simpler computational graph. Taking the idea of this work can we develop a stable dynamic system but which does not only have one attractor?It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example what is the average relaxation time of the whole hidden units at each layer?Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.The quality of the work is good explanation is clear enough along with nice analyses and proofs. Overall the performance is not any better than LSTMs but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task e.g. translation. Figure 4 of this paper is very interesting where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.,A recurrent neural network without chaos,4.0081,True,['male' 'male']
The reviewers all enjoyed this paper and the analysis.  pros: - novel new model - interesting insights into the design of model through analysis of trajectories of hidden states of RNNs.  cons: - results are worse than LSTMs.,A recurrent neural network without chaos,0.6463000000000002,True,['male' 'male']
Thanks for a very interesting read.What happens if instead of driving the LSTMs with x_t = 0 you drive it with a fixed input like the word "What"? Would that behave the same as in fig 3?If you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4) do you still see chaos? If there is gradual decay in the hidden units' activations do you also see that the second layer forgets more slowly than the first?Have you tried training on the copy task as in the algorithmic learning literature (like NTM) to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?,A recurrent neural network without chaos,0.12210000000000011,True,['male' 'male']
The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.This paper is refreshing. Instead of proposing another incremental improvement the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.,A recurrent neural network without chaos,0.5613,True,['male' 'male']
I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work one that worth exploring. ,A recurrent neural network without chaos,1.3199999999999998,True,['male' 'male']
We added a short conclusion reflecting some of the discussions with the reviewers. ,A recurrent neural network without chaos,0.0,True,['male' 'male']
Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).We added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.Importantly this behavior (i.e. higher layers decay more slowly) can be explained analytically see equation (11).Overall we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM.,A recurrent neural network without chaos,0.7968000000000001,True,['male' 'male']
I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks) i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically it would be nice to see if your network gets closer to this edge during training (I think it will).It wasn't clear to me if you studied the chaoticity in the case *with* input... the "epsilon-activation" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero probably starting from negative values in your case).The LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a) but more noisy because there's effects from the input.Anyway overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective.,A recurrent neural network without chaos,-0.4889999999999998,True,['male' 'male']
Authors' response well answered my questions. Thanks. Evaluation not changed.###This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods the model avoids manually annotating subtrees with special tokens and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset and outperforms alternative methods on one real-world IFTTT dataset. There are couple of interesting results in the paper that I believe is worth further investigation. Firstly on the synthetic dataset the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input i.e. large tree structure? I believe that it is important to understand this before a better model can be developed. For example if it is the fault of encoder maybe an attention layer can be added as in a seq-to-seq model to preserve more information of the input sequence. Moreover besides only showing how the precision changes with the number of nodes in the tree it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover as greedy search is used in decoding it might be interesting to see how it helps if it does to use beam-search in tree decoding. On the IFTTT dataset listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?The paper is well written except for minor typo as mentioned in my pre-review questions. In general I believe this is a solid paper and more can be explored in this direction. So I tend to accept it.,Tree-structured decoding with doubly-recurrent neural networks,3.574300000000001,True,['male' 'male']
The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting and the problem is quite important and general. Though the experiments are somewhat limited reviewers generally believe that they are sufficient to show that the approach holds a promise.  + an important and under-explored setting + novel model + well written  - experimentation could be stronger (but seems sufficient -- both on real and artificial data),Tree-structured decoding with doubly-recurrent neural networks,1.2015,True,['male' 'male']
It is really a nice work and paper is written quite well. The related work section is comprehensive and the problem is well motivated. And in my view the experiments are good enough especially the paper contribution is introducing a new model which can be very useful in generating structured outputs using recurrent structure.Questions: q1) How long did it take to train each of the networks in the paper?q2) Wondering any plan to release the code?Thanks. ,Tree-structured decoding with doubly-recurrent neural networks,2.3625,True,['male' 'male']
This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data.One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application and there is also a synthetic dataset however it would be more interesting to see more natural language applications with syntactic tree structures. Still I consider the experiments sufficient as a first step to showcase a novel architecture.A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture about when / how to decide to terminate as opposed to making a single arbitrary choice.I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.,Tree-structured decoding with doubly-recurrent neural networks,2.3413999999999997,True,['male' 'male']
The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches — (1) the information flows in two directions both from the parent and from siblings which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.I think the recovering synthetic tree task is not very satisfying for two reasons — (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3 when the number of nodes grows (even to a number not very large) the performance of the model drops dramatically I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case seems can’t show its full potentials since the length of the information flow in the model won’t be very long.I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example we have the seq2seq parsing model (Vinyals et al 2014) is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.,Tree-structured decoding with doubly-recurrent neural networks,2.2285999999999997,True,['male' 'male']
Authors' response well answered my questions. Thanks. Evaluation not changed.###This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods the model avoids manually annotating subtrees with special tokens and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset and outperforms alternative methods on one real-world IFTTT dataset. There are couple of interesting results in the paper that I believe is worth further investigation. Firstly on the synthetic dataset the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input i.e. large tree structure? I believe that it is important to understand this before a better model can be developed. For example if it is the fault of encoder maybe an attention layer can be added as in a seq-to-seq model to preserve more information of the input sequence. Moreover besides only showing how the precision changes with the number of nodes in the tree it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover as greedy search is used in decoding it might be interesting to see how it helps if it does to use beam-search in tree decoding. On the IFTTT dataset listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides?The paper is well written except for minor typo as mentioned in my pre-review questions. In general I believe this is a solid paper and more can be explored in this direction. So I tend to accept it. ,Tree-structured decoding with doubly-recurrent neural networks,3.574300000000001,True,['male' 'male']
A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.Summary:———I think the discussed insights are very interesting but not presented convincingly. Firstly claims are emphasized which are often violated in practice (e.g. no convergence guarantees due to mini-batches) statements could be validated more convincingly (e.g. is monotone convergence a curse or a blessing) the experimental evaluation should be extended. In summary I think the paper requires some more attention to form a compelling story.Quality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges e.g. back propagation is contrasted with CCCP.Clarity: Some of the derivations and intuitions could be explained in more detail.Originality: The suggested idea is reasonable albeit heuristics are required.Significance: Since the experimental setup is somewhat limited according to my opinion significance is hard to judge at this point in time.Details:————1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly e.g. mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.2. In spirit similar is work by B. Amos and J. Kolter Input-Convex Deep Networks (,Trusting SVM for Piecewise Linear CNNs,2.5682,True,['male' 'male' None]
The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer reduces the sub-problem into latent structured SVM.   Reviewers mainly expressed concerns about the experimental results which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews I believe the changes made should have been sufficient for them to do so.  Thus I recommend this paper be accepted.,Trusting SVM for Piecewise Linear CNNs,1.1568,True,['male' 'male' None]
tldr: New results on ImageNet CIFAR-100 improved results on CIFAR-10.We thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).List of changes in version 2:1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)List of changes in version 3:1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)2) New results on CIFAR-100 (subsection 5.2)3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).5) Infeasibility of standard line-search in Introduction6) New references including suggestions from the reviewers (section 2)7) More compact abstract8) Inclusion of batch normalization in Discussion (section 6)9) Minor rewording and typo fixes throughout the paper.List of changes in version 4:1) Added ImageNet results (subsection 5.3),Trusting SVM for Piecewise Linear CNNs,2.3129999999999997,True,['male' 'male' None]
This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM which has been well-studied in the literature. In addition the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall this paper is interesting. However unfortunately the experiment is not convincing. Pros:- To my best knowledge the proposed approach is novel and the authors provide nice theoretical analysis.- The paper is well-written and easy to follow. Cons:- Although the proposed approach can be applied in general structured prediction problem the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. 	- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example ,Trusting SVM for Piecewise Linear CNNs,2.4357,True,['male' 'male' None]
This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization i.e. at each step it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that for a large class of CNNs the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature in particular concave-convex procedure to learn the parameters of CNNs.Overall the paper is well-written. Traditional CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting and might bridge the gap and facilitate the transferring of ideas between these two camps.Of course the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization you already lose something in terms of optimizing the objective (since you are using coordinate descent instead of gradient descent). Of course you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper) but to me this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).The experiment is a bit weak.1) Only CIFAR10 is used. This is a very small dataset by today's standard while CNNs are typically used in large-scale datasets such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.2) This paper only compares with a crippled variant of SGD (without batch normalization dropout etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier SGD is not designed to purely obtain the best solution that optimizes the objective the goal of SGD is to reasonably optimize the objective while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. ,Trusting SVM for Piecewise Linear CNNs,7.193000000000001,True,['male' 'male' None]
This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically A3C is used for the RL problem and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks.The paper is well written experiments are convincing and the value of the auxiliary tasks for the problem are clear. However the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks) or on auxiliary tasks with RL in general.  As it is it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation but of relatively narrow interest.,Learning to Navigate in Complex Environments,3.5281999999999996,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
The paper proposes an approach to navigating in complex environments using RL agents that have auxiliary tasks besides just the successful navigation itself (for instance the task of predicting depth from images). The idea is a nice one and the demonstration is fairly compelling. The one aspect that seems a bit unsatisfying is that the the approach does seem a bit ad-hoc and could be made more formal but presenting these results on a challenging task like this navigation problem is certainly sufficient for the paper to be worth accepting. The pros and cons are as follows:   Pros: + Idea of formulating auxiliary tasks is a nice one and the precise form in which it is done here appears novel + Good results on a challenge task of maze navigation from visual data  Cons: - Methodology does seem a bit ad-hoc it would be nice to see if some of the auxiliary task mechanisms could be formalized beyond simple "this is what worked for this domain",Learning to Navigate in Complex Environments,2.9459999999999997,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
We have addressed the points/suggestions raised through several additional experiments which have been added to the paper in the latest revision.1) Exploring the optimal combinations of auxiliary tasks: we now include an additional auxiliary task reward prediction (shown to be effective across a range of tasks in Jaderberg et al. 2016 ICLR submission). Results show that depth prediction is superior to reward prediction in the navigational settings examined with the combination of the two being no more effective (section 5.3 table 2).2) The focus of our paper on navigation and the depth prediction auxiliary task: we now present results showing that auxiliary depth prediction is beneficial in scenarios that have minimal navigational demands suggesting its general effectiveness (appendix C.3; figure 10).3) Whether auxiliary tasks increase robustness to hyperparameters: we provide a more systematic analysis to show that this does seem to be the case (appendix C.4; figure 11).4) Whether auxiliary tasks simply accelerate training: we provide evidence that they do more that this through analyses of asymptotic performance (appendix C.5; table 3) effects on the representations learnt (i.e. indexed by position decoding) demonstration that depth prediction shows benefits compared to the reward prediction auxiliary task in the navigation domain. ,Learning to Navigate in Complex Environments,3.2375,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison but the paper could have included some more insights such as the change in representation with and without auxiliary training. I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels which is clearly a supervised learning task!,Learning to Navigate in Complex Environments,2.8046999999999995,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information such priors are well suited to a large variety of tasks pertaining to navigation and robotics.Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed.Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model which would not be without the auxiliary tasks.While specific to 3D-environment tasks this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original clearly presented and strongly supported by empirical evidence.One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters.Another downside is that the authors dismiss navigation literature as "not RL". I sympathize with the limit on the number of things that can fit in a paper but some experimental comparison with such literature may have proven insightful if just in measuring the quality of the learned representations.,Learning to Navigate in Complex Environments,4.2278,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
We have just submitted an updated version of the paper including additional references as well as new results on those agents that are enhanced with auxiliary tasks.Specifically we investigated:1) a new way of performing depth prediction by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.2) the use of depth prediction as an auxiliary task for the policy LSTM instead of the convnet.3) the effect during actor critic training of reward clipping on the performance of the agent as well as on the stability of RL learning.,Learning to Navigate in Complex Environments,0.8451,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
I like the approach (and more thorough insights) into making the RL problem easier by providing additional related SL tasks to learn; however I think that "Recurrent Reinforcement Learning: A Hybrid Approach" should be cited as previous work in this regard (on top of Lample & Chaplot).,Learning to Navigate in Complex Environments,0.5267,True,['male' 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'male' 'male' 'male' 'female']
This paper proposes a novel method for extracting rule-based classifiers from trained LSTM models. The proposed method is applied to a factoid question-answering task where it is demonstrated that the extracted rules perform comparatively to the original LSTM. The analysis of the extracted rules illustrate the features the LSTM model picks up on.Analyzing and visualizing the computations carried out by RNNs in order to understand the functions they compute is an important direction of research. This sort of analysis will help us understand the pitfalls of RNNs and how we can improve them. Although the approach taken is relatively inflexible - each rule is defined as an ordered sequence of words - the authors experiment with three different scores for picking salient words (state-difference cell-difference and gradient) and their approach yields comparable performance which suggests that the extracted rules mimic the RNN closely. The results are also somewhat surprising since most of the rules consist only of two or three words.It would have been interesting to try extend the approach on other natural language processing tasks such as machine translation. Presumably the rules learned here will be quite different.Other comments:- Eq. (12) is over-parametrized with two vectors $P$ and $Q$. The same function can be computed with a single vector. This becomes clear when you divide both the numerator and denominator by $e^{P h_t}$.- Section 4.1. Is it correct that this section is focused on the forward LSTM? If so please clarify it in the text.- In Eq. (13) define $c_0 = 0$.- Eq. (13) is exactly the same as Eq. (15). Is there a mistake?- In Table 1 third column should have word "film" highlighted.- "are shown in 2" -> "are shown in Table 2".- Since there are some problems representing numbers it may help to replace each digit with the hashtag symbol #.,Automatic Rule Extraction from Long Short Term Memory Networks,3.5103,True,[None 'male']
The equation between (8) - (9) seems to be incorrect as the left hand side of which should be p_i.,Automatic Rule Extraction from Long Short Term Memory Networks,0.4939,True,[None 'male']
Timely topic (interpretability of neural models for NLP) interesting approach surprising results.,Automatic Rule Extraction from Long Short Term Memory Networks,0.5859,True,[None 'male']
In response to helpful comments from reviewers we have just uploaded a revision. The main changes are as follows- In response to requests for extensions to other datasets we now have results on 2 different binary sentiment analysis datasets - Stanford Sentiment Treebank and Yelp reviews- We introduced a simpler more general approach for extracting rules from LSTMs trained on document classification and demonstrate that with some easy modifications it can replace our prior more complex rule extraction mechanism on WikiMovies.- Our rules now take the form of simple phrases rather than allowing for variable-sized gaps between words as before- Our LSTM baseline on WikiMovies is now SOTA by nearly 4% and the automatically extracted patterns outperform the manual patterns in the earlier version.- We added a discussion on instances correctly classified by the LSTM but incorrectly classified by our rules-based algorithm- For simplicity we changed our WikiMovies baseline to a unidirectional LSTM and removed bidirectional LSTMs from the paper. Extension of our new approach to bidirectional LSTMs would be straightforward but we feel would add unneeded complexity to the presentationAll told we feel that these algorithms and results are simpler more powerful and more general than our prior work and we look forward to discussing them.,Automatic Rule Extraction from Long Short Term Memory Networks,1.4916,True,[None 'male']
EDIT: the revisions made to this paper are very thorough and address many of my concerns and the paper is also easier to understand. i recommend the latest version of this paper for acceptance and have increased my score.This paper presents a way of interpreting LSTM models which are notable for their opaqueness. In particular the authors propose decomposing the LSTM's predictions for a QA task into importance scores for words which are then used to generate patterns that are used to find answers with a simple matching algorithm. On the WikiMovies dataset the extracted pattern matching method achieves accuracies competitive with a normal LSTM which shows the power of the proposed approach. I really like the motivation of the paper as interpreting LSTMs is definitely still a work-in-progress and the high performance of the pattern matching was surprising. However several details of the pattern extraction process are not very clear and  the evaluation is conducted on a very specific task where predictions are made at every word. As such I recommend the paper in its current form as a weak accept but hope that the authors clarify their approach as I believe the proposed method is potentially useful for NLP researchers.Comments:- Please introduce in more detail the specific QA tasks you are applying your models on before section 3.3 as it's not clear at that point that the answer is an entity within the document.- 3.3: is the softmax predicting a 0/1 value (e.g. is this word the answer or not?)- 3.3: what are the P and Q vectors? do you just mean that you are transforming the hidden state into a 2-dimensional vector for binary prediction?- how does performance of the pattern matching change with different cutoff constant values?- 5.2: are there questions whose answers are not entities? - how could the proposed approach be used when predictions aren't made at every word? is there any extension for say sentence-level sentiment classification?,Automatic Rule Extraction from Long Short Term Memory Networks,3.8594999999999997,True,[None 'male']
This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize however it seems it will be a useful way to understand and debug models.The questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?The approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used seem to show that the automatic method is unable to deal with word types.It would also be good to see an attention model as a baseline in addition to the gradient-based baseline.Minor comments:- P and Q seem to be undefined.- Some references seem to be bad e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.- In the paragraph above section 6.3: 'adam' -> 'Adam'.,Automatic Rule Extraction from Long Short Term Memory Networks,1.9275000000000002,True,[None 'male']
Nice idea but not complete model size is not reduced by the large factors found in one of your references (Song 2016) where they go to 5 bits but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper so at the moment it is difficult to compare,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,-0.7021,True,[None None 'female' 'female' None]
The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple but novel and effective with thorough evaluation on a variety of ImageNet classification models.,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,0.7964,True,[None None 'female' 'female' None]
Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.In this updated version we carefully considered all reviewers’ suggestions to improve the paper. Generally we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.’s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization definition of bit-width detailed experimental settings and so on and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials. Moreover to make our work fully reproducible the code (along with an instruction manual) will be released to public as we promised in the paper submission.(1) To reviewer 1:Question1: “Also the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g. the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.”Following your suggestion we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.Question 2: “The paper could use another second pass for writing style and grammar.”Following your suggestion we tried our best to do a much better work on revising and proof-reading with the helps from the native colleagues in USA.(2) To reviewer 2:Thanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.’s method [1]. Furthermore detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.’s method [1] with significant margins.(3) To reviewer 3:Question 1: “1) It would be good to incorporate some of the answers into the paper mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.”Following your suggestion we incorporated related results into the paper (please see Section 3.4 for details).Question 2: “It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).”Following your suggestion we revised related parts especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).Question 3: "The "5 bits" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit e.g. 0 - other values are represented with 5 bits where the first bit is needed to distinguish from 0 and the remaining 4 bits represent the 16 different values for the powers of 2.”Following your suggestion we made a clear clarification on the definition of bit-width accordingly.References:Song Han Jeff Pool John Tran and William J. Dally. Deep compression: Compressing deep neural networks with pruning trained quantization and huffman coding. ICLR 2016.Yunchao Gong Liu Liu Ming Yang and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1 2014.,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,6.417000000000002,True,[None None 'female' 'female' None]
There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights even down to 1 or 2 bits. However so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly the results are substantially better than those achieved with other quantization approaches.Overall this paper is clear the technique is as far as I am aware novel the experiments are thorough and the results are very compelling so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g. the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,2.6833,True,[None None 'female' 'female' None]
The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine results look good and my questions have been addressed.To improve the paper:1) It would be good to incorporate some of the answers into the paper mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The "5 bits" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:- 0 is represented with 1 bit e.g. 0- other values are represented with 5 bits where the first bit is needed to distinguish from 0 and the remaining 4 bits represent the 16 different values for the powers of 2.,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,2.0679,True,[None None 'female' 'female' None]
Dear ReviewersPlease take a look through the paper and ask the authors to clarify any questions you might have. The deadline for this part of the review process is December 2 2016.Thanks!,Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights,1.092,True,[None None 'female' 'female' None]
First of all thanks for this excellent work.My question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \rho(s_t a_t) \psi(s_t a_t) (R_t^\lambda - V(s_t))With \rho(s_ta_t) = \pi(a_t | s_t) / \mu(a_t | s_t) and \psi(s_t a_t) = \grad_\theta ( log \pi (a_t | s_t) ) /  \pi (a_t | s_t)The last division by \pi (a_t | s_t) is missing in equation (4).Am I mistaken or is the reference wrong?Thanks for your time.,Sample Efficient Actor-Critic with  Experience Replay,0.22860000000000008,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
pros: - set of contributions leading to SOTA for sample complexity wrt Atari (discrete) and continuous domain problems - significant experimental analysis - long all-in-one paper  cons: - builds on existing ideas although ablation analysis shows each to be essential - long paper The PCs believe this paper will be a good contribution to the conference track.,Sample Efficient Actor-Critic with  Experience Replay,0.25,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
Dear reviewers we would really appreciate it if you can take a look at the paper again in light of our replies the updated paper and the comments from Xi Chen. Thanks very much for your time!,Sample Efficient Actor-Critic with  Experience Replay,1.1727,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.Roughly:1. Especially in deep rl policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone I believe merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (,Sample Efficient Actor-Critic with  Experience Replay,2.1358,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
We thank the three reviewers. The one common concern is ablations. This paper proposes several new ideas and then goes on to combine these ideas.  To answer the reviewers concerns about ablations we added a new figure (Figure 4). This is an extremely important figure and we urge the reviewers and readers to consult it as it should answer any concerns and highlight the value of the many contributions made in this paper. The figure shows that each ingredient (Retrace/Q-lambda with off-policy correction stochastic dueling nets and the NEW trust region method) on its own leads to a massive improvement. Likewise truncation with bias correction plays an important role for large action spaces (control of humanoid). This figure indicates that this paper is not about making 4 small contributions and combining them. Rather it is about making 4 important contributions which are all essential to obtain a stable scalable general off-policy actor critic. Attaining this has been a holy grail and this paper shows how to do it.Given our good results we could easily have written several papers; one for each contribution. Instead we chose to do the honest thing and write a single solid 20-page paper aimed at truly building powerful deep RL agents for both continuous and discrete action spaces. The paper also presents novel theoretical results for RL and a very comprehensive experimental study. We did not want to claim state-of-the-art on Atari because this often depends on how one chooses to measure what should be state-of-the-art (eg sample complexity highest median highest mean etc.). But clearly in terms of median ACER with 1 replay achieves a higher median score that any previously reported result. Note that this result is not just for a few games but for the entire set of 57 games. The UNREAL agent submitted to this conference is the only method we know that achieves a higher median but it does so by adding auxiliary tasks to A3C and massive hyper-parameter sweeps. We could also add auxiliary tasks to ACER and do hyper-parameter sweeps to further improve it but this is left for future work as we wanted to focus on designing a powerful core RL agent.We hope this reply and in particular the ablations clearly answer your concerns. With 3 6’s this thorough paper will be rejected despite the several novel contributions it makes new theoretical analysis and excellent results on a comprehensive set of tasks. We hope you take the ablations and this reply into consideration to choose your final scores. ,Sample Efficient Actor-Critic with  Experience Replay,6.858900000000001,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
This paper studies the off-policy learning of actor-critic with experience replay. This is an important and challenging problem in order to improve the sample efficiency of the reinforcement learning algorithms. The paper attacks the problem by introducing a new way to truncate importance weight a modified trust region optimization and by combining retrace method. The combination of the above techniques performs well on Atari and MuJoCo in terms of improving sample efficiency. My main comment is how does each of the technique contribute to the performance gain? If some experiments could be carried out to evaluate the separate gains from these tricks it would be helpful. ,Sample Efficient Actor-Critic with  Experience Replay,2.8956,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
This paper introduces an actor-critic deep RL approach with experience replay which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems where it yields performance comparable to state-of-the-art methods.As mentioned in the beginning of the paper the main contributions of this work lies in combining 1) truncated importance sampling with retrace 2) trust region policy optimization and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.However each improvement appears to be quite incremental. Moreover the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain I would still put my money on prioritized replay due to its simplicity. Thirdly improving sample efficiency for deep RL is a laudable goal but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.Some technical aspects which need clarifications:- For Retrace I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?- The paper states that it uses a replay memory of 50000 frames so that across threads it is comparable in size to previous work. However for each thread this is much smaller compared to earlier experiments on Atari games. For example one million experience replay transitions were used in the paper "Prioritized Experience Replay" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work the authors need to also experiment with larger experience replay memories.Other comments:- Please move Section 7 to the appendix.- "Moreover when using small values of lambda to reduce variance occasional large importance weights can still cause instability": I think what is meant is using *large* values of lambda.- Above eq. (6) mention that the squared error is used.- Missing a "t" subscript at the beginning of eq. (9)?- It was hard to understand the stochastic duelling networks. Please rephrase this part.- Please clarify this sentence "To compare different agents we adopt as our metric the median of the human normalized score over all 57 games."- Figure 2 (Bottom): Please add label to vertical axes.,Sample Efficient Actor-Critic with  Experience Replay,4.721800000000001,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
The paper looks at several innovations for deep RL and evaluates their effect on solving games in the Atari domain.  The paper reads a bit like a laundry list of the researcher’s latest tricks.  It is written clearly enough but lacks a compelling message.  I expect the work will be interesting to people already implementing deep RL methods but will probably not get much attention from the broader community.The claims on p.1 suggest the approach is stable and sample efficience and so I expected to see some theoretical analysis with respect to these properties. But this is an empirical claim; it would help to clarify that in the abstract.The proposed innovations are based on sound methods.  It is particularly nice to see the same approach working for both discrete and continuous domains.The paper has reasonably complete empirical results. It would be nice to see confidence intervals on more of the plots. Also the results don’t really tease apart the effect of each of the various innovations so it’s harder to understand the impact of each piece and to really get intuition for example about why ACER outperforms A3C.  Also it wasn’t clear to me why you only get matching results on discrete tasks but get state-of-the-art on continuous tasks.The paper has good coverage of the related literature. It is nice to see this work draw more attention to Retrace including the theoretical characterization in Sec.7.,Sample Efficient Actor-Critic with  Experience Replay,4.3829,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
Should the inside summations of equation (3) should go from i = 0 to (k - t)?,Sample Efficient Actor-Critic with  Experience Replay,0.0,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
Dear AuthorsPlease resubmit your paper in the ICLR 2017 format with the correct marging spacing for your submission to be considered. Thank you!,Sample Efficient Actor-Critic with  Experience Replay,1.0193,True,[None 'male' 'male' 'male' 'male' 'male' 'male']
The paper discuss a "batch" method for RL setup to improve chat-bots.The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. I find the writing clear and the algorithm a natural extension of the online version.Below are some constructive remarks:- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why and add this to the discussion. Here is one option:- For the artificial task it seems like you are giving the constant value function an unfair advantage as it can update all the weights of the model and not just the top layer like the per-state value function.- section 2.2:   sentence before last: s' is not defined.    last sentence: missing "... in the stochastic case." at the end.- Section 4.1 last paragraph: "While Bot-1 is not significant ..." => "While Bot-1 is not significantly different from ML ...",Batch Policy Gradient  Methods for  Improving Neural Conversation Models,2.1008999999999998,True,[None 'male' 'male' 'male' 'male']
This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy.  The paper is clear well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way) this is still an interesting and informative read and will probably be of interest to many people at ICLR.,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,1.6353999999999997,True,[None 'male' 'male' 'male' 'male']
Once again we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.@Reviewer2:s' is defined in the first para of section 2.2. Also the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,1.5106000000000002,True,[None 'male' 'male' 'male' 'male']
This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model however such scores are expensive. Thus it is natural to use off-policy learning – training a base policy on unsupervised data deploying that policy to collect human scores and then learning off-line from those scores.While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation) the approach is well-motivated and the paper is written clearly and is easy to understand. My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset but this is mostly because they map directly from dialogue states to surface form rather than some embedding representation of the context. Thus it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.References:Wen Tsung-Hsien Milica Gasic Nikola Mrksic Lina M. Rojas-Barahona Pei-Hao Su Stefan Ultes David Vandyke and Steve Young. "A Network-based End-to-End Trainable Task-oriented Dialogue System." arXiv preprint arXiv:1604.04562 (2016).,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,1.4624,True,[None 'male' 'male' 'male' 'male']
The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.The approach is well motivated and the paper is well written except for some intuitions for why the batch version outperforms the on-line version (see comments on "clarification regarding batch vs. online setting").The artificial experiments are instructive and the real-world experiments were performed very thoroughly although the results show only modest improvement. ,Batch Policy Gradient  Methods for  Improving Neural Conversation Models,1.6343,True,[None 'male' 'male' 'male' 'male']
The paper discuss a "batch" method for RL setup to improve chat-bots.The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. I find the writing clear and the algorithm a natural extension of the online version.Below are some constructive remarks:- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why and add this to the discussion. Here is one option:- For the artificial task it seems like you are giving the constant value function an unfair advantage as it can update all the weights of the model and not just the top layer like the per-state value function.- section 2.2:   sentence before last: s' is not defined.    last sentence: missing "... in the stochastic case." at the end.- Section 4.1 last paragraph: "While Bot-1 is not significant ..." => "While Bot-1 is not significantly different from ML ...",Batch Policy Gradient  Methods for  Improving Neural Conversation Models,2.1008999999999998,True,[None 'male' 'male' 'male' 'male']
This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs LSTMs feed-forward networks and random forests (making a case for why random forests should be used instead of SVMs) and analysed the predictions and embeddings.The authors also did address the questions of the reviewers.My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,-0.5719,True,['male' 'male']
This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty it is well done and well organized. It demonstrates a creative use of recent models in a very important domain and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover I think an advantage to accepting it at ICLR is that it gives our "expert" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences so I think it would be helpful for those domains to have examples of application papers that are considered sound.,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,2.0305999999999997,True,['male' 'male']
This is a well written organized and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture it instead addressed an important application of predicting the medications a patient is using given the record of billing codes.  The dataset they use is impressive and useful and frankly more interesting than the typical toy datasets in machine learning.  That said the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application I was encouraged to see the authors use cutting edge choices (eg Keras adadelta etc) in their architecture.  A few points of criticism:-The numerical results are in my view too brief.  Fig 4 is anecdotal Fig 5 is essentially a negative result (tSNE is only in some places interpretable) so that leaves Table 1.  I recognize there is only one dataset but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.- To be constructive re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative other than as a negative result which I think can be stated in a sentence rather than in a large figure.- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is as it stands now researchers without medical experience have to take your results after Table 1 on faith rather than getting to apply their well-trained quantitative eye). Overall a nice paper.,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,3.4004,True,['male' 'male']
In light of the detailed author responses and further updates to the manuscript I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.-----This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs similar to that described in related work by Choi et al. In experiments using a cohort of ~610K patient records they find that RNN models outperform strong baselines including an MLP and a random forest as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant given the size of the test set.Strengths:- Very important problem. As the authors point out two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.- Paper is well-written with lucid introduction and motivation thorough discussion of related work clear description of experiments and metrics and interesting qualitative analysis of results.- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers including Lipton & Kale et al ICLR 2016 where the gap between the RNN and MLP was relatively small and Choi et al MLHC 2016 which omitted many obvious baselines.- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.Weaknesses:- The authors make several unintuitive decisions related to data preprocessing and experimental design foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results but it is somewhat unnatural and the explanation is difficult to follow reducing the paper's potential impact. It is also reduces the RNN's potential advantage.- The chosen metrics seem appropriate but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial e.g. RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are "significant" (even in an informal sense).- The paper proposes nothing novel in terms of methods which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless but there may be things the authors can do to make it more competitive. For example one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this perhaps by looking at the kinds of errors each model makes?I have a final comment: as a piece of clinical work the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training the authors are right that this shouldn't be a huge problem provided the label noise is random (even class conditional isn't too big of a problem). For testing though this seems like it could skew metrics. Further the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.For what it is worth I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.,Predicting Medications from Diagnostic Codes with Recurrent Neural Networks,6.074699999999999,True,['male' 'male']
This paper proposes the graph convolutional networks motivated from approximating graph convolutions.  In one propagation step what the model does can be simplified as first linearly transform the node representations for each node and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added) and then pass through nonlinearity.This model is used for semi-supervised learning on graphs and in the experiments it demonstrated quite impressive results compared to other baselines outperforming them by a significant margin.  The evaluation of propagation model is also interesting where different variants of the model and design decisions are evaluated and compared.It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments this is really surprising as a two-layer model is very local and the output of a node can only be affected by nodes in a 2-hop neighborhood and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3) I wonder if adding more layers helped anything or not.Even though motivated from graph convolutions when simplified as the paper suggests the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016 the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?Overall I think this model is simple but the connection to graph convolutions is interesting and the experiment results are quite good.  There are a few questions that still remain but I feel this paper can be accepted.,Semi-Supervised Classification with Graph Convolutional Networks,4.5737,True,['male' 'male']
The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.,Semi-Supervised Classification with Graph Convolutional Networks,0.7096,True,['male' 'male']
Dear ReviewersThanks a lot for reviewing our paper and for your valuable comments. To incorporate your feedback we have uploaded a revision of our paper with the following changes:1) We have added the Iterative Classification Algorithm (ICA) from Lu & Getoor (2003) as a baseline as suggested by Reviewer 2. Thanks a lot for pointing out the references on iterative classification. ICA is indeed a powerful baseline that we have not considered previously and it compares favorably against some of the other baselines. We have put the code to reproduce the ICA baseline experiments on Github: ,Semi-Supervised Classification with Graph Convolutional Networks,1.9226,True,['male' 'male']
The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ,Semi-Supervised Classification with Graph Convolutional Networks,0.5750000000000001,True,['male' 'male']
The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.The paper is technically fine and clear the algorithm seems to scale well and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper but the authors could better highlight the differences and novelty wrt this reference paper. Also there is no comparison with the family of iterative classifiers which usually compare favorably both in performance and training time with regularization based approaches although they are mostly used in inductive settings. Below are some references for this family of methods.The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML Vol. 3. 496–503.Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010) 955–984.David Jensen Jennifer Neville and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM 593–598.Joseph J Pfeiffer III Jennifer Neville and Paul N Bennett. 2015. Overcoming Relational Learning Biasesto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee 853–863.Stephane Peters Ludovic Denoyer and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM) 2010 International Conference on. IEEE 96–103.,Semi-Supervised Classification with Graph Convolutional Networks,2.7391,True,['male' 'male']
This paper puts forward a not entirely new but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.,Dropout with Expectation-linear Regularization,2.0693,True,[None None 'female' None None 'male']
This paper presents a theoretical underpinning of dropout and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper "Variational Dropout and the Local Reparameterization Trick" by Diederik P. Kingma Tim Salimans Max Welling.,Dropout with Expectation-linear Regularization,0.8416000000000001,True,[None None 'female' None None 'male']
We made the following revisions:1. We switched the section 6.3 and 6.4 to make the paper more clear.2. We added the definition of MC dropout on page 8.3. We fixed all the typos in the three reviewers' comments.,Dropout with Expectation-linear Regularization,0.4391,True,[None None 'female' None None 'male']
summaryThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble or here the latent variable model) and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.Experiments are performed on MNIST CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble of course quite expensive computationally)The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.p6 line 8 typo: expecatation,Dropout with Expectation-linear Regularization,2.0722,True,[None None 'female' None None 'male']
This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore they propose use of per-sample based inference gap as a regularizer and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.MC dropout on page 8 is not defined please define.On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.Couple of typos:- Pg. 2 “ … x is he input …” -> “ … x is the input …”- Pg. 5 “ … as defined in (1) is …” -> ref. to (1) is not right at two places in this paragraphOverall it is a good paper I think should be accepted and discussed at the conference.,Dropout with Expectation-linear Regularization,2.781,True,[None None 'female' None None 'male']
This paper puts forward a not entirely new but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction. ,Dropout with Expectation-linear Regularization,2.0693,True,[None None 'female' None None 'male']
This paper considers the energy-based model interpretation of GAN where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.The main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.The idea of using a kernel to push apart the sampled points is interesting and will work in low dimensions but it is hard to see how it can work in full scale images. For high dimensional samples x the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:- There is no experiment that compares between standard GAN and GAN + repulsion using the same architecture. (please address this in the rebuttal)- If the Stein variational idea is taken literally the right thing to do would be to fully optimize the generator at every step and then taking a single optimization step on the discriminator. Instead each is updated in turn and the learning rates of both steps are adjusted to keep the two "in line".- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.The authors obtain good results: The generated images clearly look better than those generated by DCGAN. However their approach has a number of changes compared to DCGAN so it is not clear where the improvement comes from. In addition by now the DCGAN is no longer a very strong baseline as various other techniques have been proposed.Note: The use of phi for both the "particle gradient direction" and energy function is confusing,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,3.5453,False,[None 'male']
This paper presents an idea with a sensible core (augmenting amortized inference with per-instance optimization) but with an overcomplicated and ad-hoc execution. The reviewers provided clear guidance for how this paper could be improved and thus I invite the authors to submit this paper to the workshop track.,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,0.8457,False,[None 'male']
We highly appreciate the time and feedback from all the reviewers all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: [Testing Accuracy Score]We agree with the reviewers' point on the "testing accuracy" score but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the "effective amount" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. [Repulsive Term in High Dimension]Our repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations because the kernel only defines the "tangent space" for improvement. SteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34 while that SteinGAN with kernel gives 6.76. [Amortized is slower than non-amortized]Although the amortized algorithm has the overhead of updating $\xi$ it stores the information in a generative network and allows us to simulate as many images as we need. By using the one-step gradient update we proposed the update of $\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. ,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,4.0875,False,[None 'male']
This paper proposes an amortized version of the Stein variational gradient descent (SVGD) method in which "a neural network is trained to mimic the SVGD dynamics". It applies the method to generative adversarial training to yield a training procedure where the discriminator is interpreted as an energy-based probabilistic model.One criticism I have of the presentation is that a lot of time and energy is spent setting the table for a method which is claimed to be widely applicable and the scope of the empirical evaluation is narrowed down to a single specific setting. In my view either the paper falls short of its goal of showing how widely applicable the proposed method is or it spends too much time setting the table for SteinGAN and not enough time evaluating it.The consequence of this is that the empirical results are insufficient in justifying the approach proposed by the paper. As another reviewer pointed out DCGAN is becoming outdated as a benchmark for comparison.Qualitatively SteinGAN samples don't look significantly better than DCGAN samples except for the CelebA dataset. In that particular case the DCGAN samples don't appear to be the ones presented in the original paper; where do they come from?Quantitatively DCGAN beats SteinGAN by a small margin for the ImageNet Inception Score and SteinGAN beats DCGAN by an even smaller margin for the CIFAR10 Inception Score. Also in my opinion the "testing accuracy" score is not a convincing evaluation metric: while it is true that it measures the amount of information captured in the simulated image sets it is only sensitive to information useful for the discrimination task not for the more general modeling task. For instance this score is likely completely blind to information present in the background of the image.Because of the reasons outlined above I don't think the paper is ready for publication at ICLR.,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,0.05279999999999996,False,[None 'male']
The authors propose amortized SVGD an amortized form of prior work on SVGD which is a particle variational method that maximally decreases the KL divergence at each update. "amortized SVGD" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models which admit a tractable unnormalized density.In SVGD the main difference from just MAP is the addition of a "repulsive force" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work this was not a problem as the only experiments were on toy and UCI data sets.In the experimental results here they apply the kernel on the hidden representation of an autoencoder which seems key similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder which changes per iteration) before then applying their method.Unlike previous literature which uses inference networks their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches this would be like having to perform local inferences before then updating inference network parameters or at least partially performing the local inference). This seems quite costly during training.I recommend the paper be rejected and that the authors provide more comprehensive experimental results expecially around the influence of the autoencoder the incremental updates versus full updates and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.ReferencesLi Y. Swersky K. & Zemel R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning.,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning,3.4986000000000006,False,[None 'male']
This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.The paper is well written and the use of traces in deep RL is indeed underexplored but the experiments in the paper are too limited and do not answer the most interesting questions.As pointed out in the questions n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.The other claimed contribution of the paper is showing the strong effect of optimization. As with traces I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.[1] "Asynchronous Methods for Deep Reinforcement Learning" ICML 2016.,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,3.4726999999999997,False,['female' 'female']
The reviewers agree that the paper is clear and well-written but all reviewers raised significant concerns about the novelty of the work since the proposed algorithm is a combination of well-known techniques in reinforcement learning. It is worth noting that the use of eligibility traces is not very heavily explored in the deep reinforcement learning literature but since the contribution is primarily empirical rather than conceptual and algorithmic there is a high bar for the rigorousness of the experiments. The reviewers generally did not find the evaluation to be compelling enough in this regard. Based on this evaluation the paper is not ready for publication.,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,0.5158999999999998,False,['female' 'female']
This paper combines DRQN with eligibility traces and also experiment with the Adam optimizer for optimizing the q-network. This direction is worth exploring and the experiments demonstrate the benefit from using eligibility traces and Adam on two Atari games. The methods themselves are not novel. Thus the primary contributions are (1) applying eligibility traces and Adam to DRQN and (2) the experimental evaluation. The paper is well-written and easy to understand.The experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do. However with only two Atari games in the results it is difficult to tell how well it the method would perform more generally. Showing results on several more games and/or other domains would significantly improve the paper. Showing error bars from multiple random seeds would also improve the paper.,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,1.8584999999999998,False,['female' 'female']
The paper presents a deep RL with eligibility traces. The authors combine DRQN with eligibility traces for improved training. The new algorithm is evaluated on a two problems with a single set of hyper-parameters and compared with DQN.The topic is very interesting. Adding eligibility traces to RL updates is not novel but this family of the algorithms have not been explored for deep RL. The paper is written clearly and the related literature is well-covered. More experiments would make this promising paper much stronger. As this is an investigative experimental paper it is crucial for it to contain a wider range of problems different hyper-parameter settings and comparison with vanilla DRQN Deepmind's DQN implementation as well as other state of the art methods. ,Investigating Recurrence and Eligibility Traces in Deep Q-Networks,1.3512,False,['female' 'female']
This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.Pros:- Important analysis- Good visualizationsCons:- The paper describes mostly the observation that the optima vary for different methods however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)- Some fonts are very small (e.g. Fig. 5),An Empirical Analysis of Deep Network Loss Surfaces,1.7519,False,['male' 'male' 'female']
The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.  The extensive experiments conducted by the authors are interesting and inspiring. However several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore theoretical analysis and analytical derivations consistent with the simulated datasets could shed light on the experimental results and allow to make more precise claims.   A revised version following the reviewers' suggestions will result in a stronger submission to a future venue.,An Empirical Analysis of Deep Network Loss Surfaces,2.5356,False,['male' 'male' 'female']
First of all I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below) I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.Major points:-- "As we saw in the previous section the minima of deep network loss functions are for the most part decent."All you said in the previous section was that theory shows that there are no bad minima under "strong assumptions". There is no practical proof that minima do not vary in quality.-- "This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent then the task of finding a "decent minima quickly" is reduced to the task of finding any minima quickly."First of all as one of the reviewers pointed out we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level in practice actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore the task of optimizing neural networks is far from being "reduced to finding any minima quickly".-- Figure 1 I don't like Figure 1 because it suggests to me that you diagnosed exactly where the transition between the two phases happened which I don't think you did. Also the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.-- Misuse of the transient phase / minimization phase conceptIn section 4.3 you talk about the transient and minimization phase of optimization. However you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur one would have to verify for example that the learning rate is small enough. You change the algorithm after 25% 50% and 75% of training but these points seem arbitrary. What is the minimization phase was reached at 99% or 10 epochs after you decided to stop training?-- Only 1 dataset You run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset. -- Many figures are unclear For each figure the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train test or validation error. It should be easy for the reader to ascertain this information for all figures not just for some. -- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.  -- Lack of confidence intervalsThe value of Figures 1 2 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1 2 3 and 6 might look like I think those confidence intervals should still be shown for at least a subset of the configurations presented.-- Lack of information regarding learning rateThere is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.-- Lack of information regarding the absolute distance of interpolated pointsIn most figures you interpolate between two or three trained weight configuration. However you do not say how far the interpolated points are apart. This is highly significant because if points are close together and there is a big "hump" between them it means that those points are more "brittle" than if they are far apart and there is a big "hump" between them. Minor points: -- LSTM is not a fixed network architecture like NiN or VGG but a layer type. LSTM would be equivalent to CNN. Also the VGG paper has multiple versions of VGG. You should specify which one you used.  -- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write "best viewed in zoom" in the table caption and pretend that somehow fixes the problem. Personally I prefer no legend over an unreadable legend.,An Empirical Analysis of Deep Network Loss Surfaces,0.5074999999999996,False,['male' 'male' 'female']
The paper is dedicated to better understanding the optimization landscape in deep learning in particular when explored with different optimization algorithms and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example there are probably plenty of ways to modify approach of Goodfellow et al. (2015) and similar works and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms gaining more intuition how the optimization surface looks like in general etc.? This is an interesting paper though I am fairly confident it is a better fit for the journal than this conference. It would be interesting and instructive even for sanity check to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.,An Empirical Analysis of Deep Network Loss Surfaces,3.6127000000000002,False,['male' 'male' 'female']
I appreciate the work but I do not think the paper is clear enough. Moreover the authors say "local minimia" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. The authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. It is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases the place/solution where they fail to make progress is not necessarily a local minimum. Some sentences like the one given below suggest that the study is too superficial:  "One of the interesting empirical observation is that we often observe is that the incremental improvementof optimization methods decreases rapidly even in non-convex problems.",An Empirical Analysis of Deep Network Loss Surfaces,0.9438,False,['male' 'male' 'female']
This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.Pros:- Important analysis- Good visualizationsCons:- The paper describes mostly the observation that the optima vary for different methods however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)- Some fonts are very small (e.g. Fig. 5),An Empirical Analysis of Deep Network Loss Surfaces,1.7519,False,['male' 'male' 'female']
This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these "deep" seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.The paper is sometimes hard to read due to (a) many grammatical errors and (b) sloppy notation in some places (e.g. on page 5 line 1 f is used but never introduced before). Overall I give an accepting score but a weak one because of the grammatical errors. If the paper is accepted these should be fixed for the final version optimally by a native speaker. The paper's topic is interesting and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular uncertainty sampling did very well (in fact better than the authors' new method) on MNIST but apparently horribly on USPS; some explanation for this would be useful.I have one more question: why is it necessary to first sample a larger subset D \subset U from which we select using active learning? Is this merely done for reasons of computational efficiency or can it actually somehow improve results? (If so it would be instrumental to see the worse results when this is not done.),Introducing Active Learning for CNN under the light of Variational Inference,2.0016,False,['female' 'male']
The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation the link between depth and need for active example selection is alluded to but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.,Introducing Active Learning for CNN under the light of Variational Inference,1.6844,False,['female' 'male']
The paper proposes to perform active learning using pool selection of deep learning mini-batches using an approximation of the bayesian posterior. Several terms are in turn approximated.The Maximum Likelihood Estimation (MLE) bayesian inference approach to active learning the various approximations and more generally the theoretical framework is very interesting but difficult to follow.The paper is written in poor English and is sometimes a bit painful to read.Alternative Active learning strategies and techniques do not need to be described with such detail. On the other hand the proposed approach has a lot of complex approximations which would benefit from a more detailed/structured presentation. Another dataset would be a big plus (both datasets concern gray digits and USPS and are arguably somewhat similar).,Introducing Active Learning for CNN under the light of Variational Inference,0.36839999999999984,False,['female' 'male']
Quality:The paper initiates a framework to incorporate active learning into the deep learning framework mainly addressing challenges such as scalability that accompanies the training of a deep neural network. However I think the paper is not well polished; there are quite a lot of grammatical and typing errors.Clarity:The paper needs major improvements in terms of clarity. The motivations in the introduction i.e. why it is difficult to do active learning in deep architectures could be better explained and tied to the explanation in Section 3 of the paper. For example the authors motivated the need of (mini)batch label queries but never mention it again in Section 3 when they describe their main methodology. The related work section although appearing systematic and thorough is a little detached from the main body of the paper (related work section should not be a survey of the literature but help readers locate your work in the relevant literature and highlight the pros and cons. In this perspective maybe the authors could shorten some explanations over the related work that are not directly related while spending more time on discussing/comparing with works that are most related to your current work e.g. that of Graves '11.Originality & Significance:The authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data where the informativeness is defined by the variational free energy which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix the authors proposed techniques to approximate it (which seems to be novel)I think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems via active learning. But the paper needs to be improved in terms of presentation.,Introducing Active Learning for CNN under the light of Variational Inference,3.74,False,['female' 'male']
This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these "deep" seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.The paper is sometimes hard to read due to (a) many grammatical errors and (b) sloppy notation in some places (e.g. on page 5 line 1 f is used but never introduced before). Overall I give an accepting score but a weak one because of the grammatical errors. If the paper is accepted these should be fixed for the final version optimally by a native speaker. The paper's topic is interesting and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular uncertainty sampling did very well (in fact better than the authors' new method) on MNIST but apparently horribly on USPS; some explanation for this would be useful.I have one more question: why is it necessary to first sample a larger subset D \subset U from which we select using active learning? Is this merely done for reasons of computational efficiency or can it actually somehow improve results? (If so it would be instrumental to see the worse results when this is not done.),Introducing Active Learning for CNN under the light of Variational Inference,2.0016,False,['female' 'male']
Some of the approximations are quite complex; is your code available?,Introducing Active Learning for CNN under the light of Variational Inference,0.0,False,['female' 'male']
In Figures 1 and 2 what do you mean by groundtruth?,Introducing Active Learning for CNN under the light of Variational Inference,0.0,False,['female' 'male']
Section 5.2 studies the time complexity of your approach -- up to 30s to select the elements of one minibatch. How does this compare to the time required for using that minibatch to update the model by backprop?,Introducing Active Learning for CNN under the light of Variational Inference,0.0,False,['female' 'male']
The manuscript is a bit scattered and hard to follow. There is technical depth but the paper doesn't do a good job explaining what shortcoming the proposed methods are overcoming and what baselines they are outperforming. The writing could be improved. There are numerous grammatical errors.The experiments in 3.1 are interesting but you need to be clearer about the relationship of your ResCeption method to the state-of-the-art. The use of extensive footnotes on page 5 is a bit odd. "That is a competitive result" is vague. A footnote links to ",Multi-label learning with the RNNs for Fashion Search,-0.47079999999999994,False,['male']
Three knowledgable reviewers recommend rejection. The main concern is missing related work on fashion product search and thus also baselines. The authors did not post a rebuttal to address the concerns. The AC agrees with the reviewers' recommendation.,Multi-label learning with the RNNs for Fashion Search,-0.3437,False,['male']
The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. However there are several concerns.1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg’s group on fashion recognition and fashion attributes e.g. -  “Automatic Attribute Discovery and Characterization from Noisy Web Data” ECCV 2010 - “Where to Buy It: Matching Street Clothing Photos in Online Shops” ICCV 2015- “Retrieving Similar Styles to Parse Clothing TPAMI 2014etcIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?3) There are some missing baselines. How do the results in Table 2 compare to simpler methods e.g. the BM or CM methods described in the text?While the paper presents an interesting exploration all these concerns would need to be addressed before the paper can be ready for publication.,Multi-label learning with the RNNs for Fashion Search,1.318,False,['male']
This paper introduces a pratical large-scale visual search system for a fashion site. It uses RNN to recognize multi-label attributes and uses state-of-art faster RCNN to extract features inside those region-of-interest (ROI). The technical contribution of this paper is not clear. Most of the approaches used are standard state-of-art methods and there are not a lot of novelties in applying those methods. For multi-label recognition task there are other available methods e.g. using binary models changing cross-entropy loss function etc. There aren't any comparison between the RNN method and other simple baselines. The order of the sequential RNN prediction is not clear either. It seems that the attributes form a tree hierarchy and that is used as the order of sequence.The paper is not well written either. Most results are reported in the internal dataset and the authors won't release the dataset. ,Multi-label learning with the RNNs for Fashion Search,-1.1087,False,['male']
We fixed minor typographical error in author's name and Section. 4. etc.Our policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure however we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision especially e-commerce industry. ,Multi-label learning with the RNNs for Fashion Search,0.3764,False,['male']
This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also "fintunes" on test examples with active search to achieve better performance.The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.However the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.Further more the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation which is not controlled across algorithms.,Neural Combinatorial Optimization with Reinforcement Learning,2.6438,False,['male' 'male' 'male' 'male' 'male']
This was one of the more controversial submissions to this area and there was extensive discussion over the merits and contributions of the work. The paper also benefitted from ICLRs open review system as additional researchers chimed in on the paper and the authors resubmitted a draft. The authors did a great job responding and updating the work and responding to criticisms. In the end though even after these consideration none of the reviewers strongly supported the work and all of them expressed some reservations.   Pros: - All agree that the work is extremely clear going as far as saying the work is "very well written" and "easy to understand".  - Generally there was a predisposition to support the work for its originality particularly due to its "methodological contributions" and even going so far as a saying it would generally be a natural accept.  Cons: - There was a very uncommonly strong backlash to the claims made by the paper particularly the first draft but even upon revisions. One reviewer even saying this was an "excellent example of hype-generation far before having state-of-the-art results" and that it was "doing a disservice to our community since it builds up an expectation that the field cannot live up to" . This does not seem to be an isolated reviewer but a general feeling across the reviews. Another faulting "the toy-ness of the evaluation metric" and the way the comparisons were carried out. - A related concern was a feeling that the body of work in operations research was not fully taken account in this work noting "operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality". The authors did fix some of these issues but not to the point that any reviewer stood up for the work.,Neural Combinatorial Optimization with Reinforcement Learning,3.5680999999999994,False,['male' 'male' 'male' 'male' 'male']
We ask reviewers to have a look at the new version of the paper again given the changes outlined below:- We state clearly in the abstract introduction and conclusion that our results are still far from the state-of-the-art (this includes adding an updated version of Figure 1 back into the introduction).- We include the original KnapSack baselines back into the paper.- We explain in details how the running time of the LKH baseline is obtained.- We modify the statement on the performance of greedy approaches: instead of stating that they are “just a few percents from optimality” we express that they are “still quite far from optimality”.We thank reviewers for their help in improving the quality of the paper.,Neural Combinatorial Optimization with Reinforcement Learning,1.1925,False,['male' 'male' 'male' 'male' 'male']
I posted this question in a response below but it seems to be getting ignored so I thought I'd bring it to the top with some additional points.Thanks for the update. The natural question to ask then is - do there exist many (or any) problems that are both interesting and have not been and cannot be addressed by the existing combinatorial optimization community? You knock existing algorithms for being "highly optimized" to particular problems but if every worthwhile problem has "highly optimized" solutions what good is your work? Also please stop calling existing TSP solvers such as concorde a heuristic. Concorde produces solutions which are provably correct. Your approach does not nor is it remotely close. From a practical perspective this is an important distinction; I don't see why anyone would choose the latter when given the choice. The second paragraphs of the related work and introduction are guilty of this. Also in the related work - you say it solves cities with "thousands of cities" when it has solved a 85k problem. I'd also echo concerns about the toy-ness of the evaluation metrics here - 100 cities is 800x smaller than existing SOTA of 85k from TSPLib - a gap made exponentially larger by the combinatorial nature of the problem.,Neural Combinatorial Optimization with Reinforcement Learning,1.4769,False,['male' 'male' 'male' 'male' 'male']
We thank reviewers for their valuable feedback that helped us improve the paper. We appreciate their interest in the method and its novelty. We have made several changes to the paper which are summarized below. We ask reviewers to evaluate the new version of the paper and adjust their reviews if necessary.1) Previous Figure 1 which was problematic due to different possible interpretations of “local search” was removed.2) We added precise running time evaluations for all of the methods in the paper. Table 3 presents running time of the RL pretraining-greedy method and the solvers we compare against. Table 4 presents the performance and corresponding running time of RL pretraining-Sampling and RL pretraining-Active Search as a function of the number solutions considered. It shows how they can be stopped early at the cost of a small performance degradation. Table 6 contains the same information for the metaheuristics from OR-Tools vehicle routing library solver. We controlled the complexity of these approaches by letting all of them evaluate 1280000 solutions. Section 5.2 was rewritten in light of the new results.3) We experimented with a new approach called RL pretraining-Greedy@16 that decodes greedily from 16 different pretrained models at inference time and selects the shortest tour. It runs as fast as the solvers while only suffering from a small performance cost.4) We added a discussion in Section 6 (Generalization to other problems) explaining how one may apply Neural Combinatorial Optimization to problems for which coming up with a feasible solution is challenging by itself.5) We added a more detailed description of the critic network (see Section 4 - Critic’s architecture for TSP).Please take a look and let us know your thoughts.,Neural Combinatorial Optimization with Reinforcement Learning,0.3967000000000001,False,['male' 'male' 'male' 'male' 'male']
This paper applies the pointer network architecture—wherein an attention mechanism is fashioned to point to elements of an input sequence allowing a decoder to output said elements—in order to solve simple combinatorial optimization problems such as the well-known travelling salesman problem. The network is trained by reinforcement learning using an actor-critic method with the actor trained using the REINFORCE method and the critic used to estimate the reward baseline within the REINFORCE objective.The paper is well written and easy to understand. Its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel. Importantly it provides an interesting research avenue for revisiting classical neural-based solutions to some combinatorial optimization problems using recently-developed sequence-to-sequence approaches. As such I think it merits consideration for the conference.I have a few comments and some important reservations with the paper:1) I take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems. The crux of combinatorial problems — for practical applications — lies in the complex constraints that define feasible solutions (e.g. simple generalizations of the TSP that involve time windows or multiple salesmen). For these problems it is no longer so simple to exclude possible solutions from the enumeration of the solution by just « striking off » previously-visited instances; in fact for many of these problems finding a single feasible solution might in general be a challenge. It would be relevant to include a discussion of whether the Neural Combinatorial Optimization approach could scale to these important classes of problems and if so how. My understanding is that this approach as presented would be mostly suitable for assignment problems with a very simple constraint structure.2) The operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality. For instance TSPLIB contains a large number of TSP instances (,Neural Combinatorial Optimization with Reinforcement Learning,0.3416999999999998,False,['male' 'male' 'male' 'male' 'male']
This paper is methodologically very interesting and just based on the methodological contribution I would vote for acceptance. However the paper's sweeping claims of clearly beating existing baselines for TSP have been shown to not hold with the local search method LK-H solving all the authors' instances to optimality -- in seconds on a CPU compared to clearly suboptimal results by the authors' method in 25h on a GPU. Seeing this clear dominance of the local search method LK-H I find it irresponsible by the authors that they left Figure 1 as it is -- with the line for "local search" referring to an obviously poor implementation by Google rather than the LK-H local search method that everyone uses. For example at NIPS I saw this Figure 1 being used in a talk (I am not sure anymore by whom but I don't think it was by the authors) the narrative being "RNNs now also clearly perform better than local search". Of course people would use a figure like that for that purpose and it is clearly up to the authors to avoid such misconceptions. The right course of action upon realizing the real strength of local search with LK-H would've been to make "local search" the same line as "Optimal" showing that the authors' method is still far worse than proper local search. But the authors chose to leave the figure as it was still suggesting that their method is far better than local search. Probably the authors didn't even think about this but this of course will mislead the many superficial readers. To people outside of deep learning this must look like a sensational yet obviously wrong claim. I thus vote for rejection despite the interesting method. ------------------------Update after rebuttal and changes:I'm torn about this paper. On the one hand the paper is very well written and I do think the method is very interesting and promising. I'd even like to try it and improve it in the future. So from that point of view a clear accept.On the other hand the paper was using extremely poor baselines making the authors' method appear sensationally strong in comparison and over the course of many iterations of reviewer questions and anonymous feedback this has come down to the authors' methods being far inferior to the state of the art. That's fine (I expected that all along) but the problem is that the authors don't seem to want this to be true... E.g. they make statements such as "We find that both greedy approaches are time-efficient and just a few percents worse than optimality."That statement may be true but it is very well known in the TSP community that it is typically quite trivial to get to a few percent worse than optimality. What's hard and interesting is to push those last few percent. (As a side note: the authors probably don't stop LK-H once it has found the optimal solution like they do with their own method after finding a local optimum. LK-H is an anytime algorithm so even if it ran for a day that doesn't mean that it didn't find the optimal solution after milliseconds -- and a solution a few percent suboptimal even faster).Nevertheless since the claims have been toned down over the course of the many iterations I was starting to feel more positive about this paper when just re-reading it. That is until I got to the section on Knapsack solving. The version of the paper I reviewed was not bad here as it at least stated two simple heuristics that yield optimal solutions:"Two simple heuristics are ExpKnap which employs brand-and-bound with Linear Programming bounds (Pisinger 1995) and MinKnap which employs dynamic programming with enumerative bounds (Pisinger 1997). Exact solutions can also be optained by quantizing the weights to high precisions and then performing dynamic programming with a pseudo-polynomial complexity (Bertsimas & Demir 2002)." That version then went on to show that these simple heuristics were already optimal just like their own method.In a revision between December 11 and 14 however that paragraph along with the optimal results of ExpKnap and MinKnap seems to have been dropped and the authors instead introduced two new poor baseline methods (random search and greedy). This was likely in an effort to find some methods that are not optimal on these very easy instances. I personally find it pointless to present results for random search here as nobody would use that for TSP. It's like comparing results on MNIST against a decision stump (yes you'll do better than that but that is not surprising). The results for greedy are interesting to see. However dropping the strong results of the simple heuristics ExpKnap and MinKnap (and their entire discussion) appears unresponsible since the resulting table in the new version of the paper now suggests that the authors' method is better than all baselines. Of course if all that one is after is a column of bold numbers for ones own approach that's what one can do but I don't find it responsible to hide the better baselines. Also why don't the authors try at least the same OR-tools solver from Google that they tried for TSP? It seems to support Knapsack directly: ,Neural Combinatorial Optimization with Reinforcement Learning,9.8054,False,['male' 'male' 'male' 'male' 'male']
This paper proposes to use RNN and reinforcement learning for solving combinatorial optimization problems. The use of pointer network is interesting as it enables generalization to arbitrary input size. The proposed method also "fintunes" on test examples with active search to achieve better performance.The proposed method is theoretically interesting as it shows that RNN and RL can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms.However the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value. The matter is further complicated by the fact that the proposed method runs on GPU while baselines run on CPU: it is hard to even come up with a meaningful unit of complexity. Money spent on hardware and electricity per instance may be a viable option.Further more the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation which is not controlled across algorithms.,Neural Combinatorial Optimization with Reinforcement Learning,2.6438,False,['male' 'male' 'male' 'male' 'male']
This is very interesting to me! Thank you for this.After reading this paper I tested the Concorde. I think the Concorde allows only integer distances(if use Euclidean distance they round off) so cannot provide optimal solution of Euclidean TSP.But error can be small if multiply the distance by a large constant.I want to know that if I correct does 'optimal' means a solution which is very closed to optimal?,Neural Combinatorial Optimization with Reinforcement Learning,0.4586999999999999,False,['male' 'male' 'male' 'male' 'male']
I am very glad to read "Our model and training code will be made available soon." Thanks for that! My question is: how soon is soon? During the review period? In time for the conference? ,Neural Combinatorial Optimization with Reinforcement Learning,1.0021,False,['male' 'male' 'male' 'male' 'male']
In Table 3 what is the performance for the missing values of RL pretraining with 10.000 batches for Sampling T=1 and T=T*? Since performance improved much more from 100 to 1.000 batches for RL pretraining Sampling T=T* than it did for RL pretraining AS (e.g. 5.79->5.71 vs 5.74->5.71 for TSP50) I would expect RL pretraining Sampling T=T* to do better than RL pretraining AS when you use 10.000 samples. This would also change your qualitative conclusion in Table 2 and the overall result of the paper. You seem to glance over this in the text by saying "we sample 1000 batches from a pretrained model afer which we do not see significant improvement" but seeing the much larger "gradient" from 50 100 and 1000 batches than for RL pretraining AS and seeing how key the result is to the final take-away from the paper I would be far more convinced by just seeing the numbers for 10.000 batches.Also what is actually the difference between RL pretraining Sampling T=1 and T=T*? (Maybe I just missed this in the text.),Neural Combinatorial Optimization with Reinforcement Learning,1.1214,False,['male' 'male' 'male' 'male' 'male']
There is a large body of work on solving TSP instances that this paper ignores. In particular the concorde algorithm has produced provably optimal solutions to problems as large as 85900 cities and can solve 100+ city problems in a few seconds on a single 500MHz core. Thus the claims made that this is even close to being a useful tool for solving TSP problems are demonstrably untrue.,Neural Combinatorial Optimization with Reinforcement Learning,0.35629999999999995,False,['male' 'male' 'male' 'male' 'male']
The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled the authors responded that subsampling is common in recommender systems work including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea and in particular doesn't answer the question of "how would the results look without subsampling" which I think is a question that could easily have been answered directly.Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper in seems odd to sample the data to reduce sparsity.Other than that the pre-review questions seem to have been answered satisfactorily.The contribution of the paper is to propose user and item embedding methods as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS though the network formulation has some differences.Overall this is an reasonably put together paper that makes a contribution in an important area though there are still some shortcomings that should be addressed namely:1) The evaluation is unusual. Recall@M is the only result reported though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness even if the results are not strong2) Given that the contribution is fairly simple (i.e. the "standard" recommender systems task but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used and exactly the same error measure for the fairest comparison possible.Without the above it's hard to tell how much the performance improvements are really due to the method being better versus the choice of datasets and the choice of loss functions.,Collaborative Deep Embedding via Dual Networks,-0.5597000000000001,False,[None None None None None]
The paper presents a collaborative filtering method using dual deep nets for users and items. The nets can take advantage of content in addition to ratings. This contribution is just below the bar in that its novelty relative to existing methods is limited and the results are good but not sufficiently impressive especially since they focus exclusively on Recall@N. In the response the authors do present results on other metrics but the results are mixed.,Collaborative Deep Embedding via Dual Networks,-0.21839999999999998,False,[None None None None None]
The authors proposed to learn embeddings of users and items by using deep neural network for a recommendation task. The resulting method has only minor differences from the previous CDL in which neural networks were also used for recommendation tasks. In the experiments since the proposed method DualNets have use more item features than WMF and CDL the comparisons are unfair. ,Collaborative Deep Embedding via Dual Networks,-0.4767,False,[None None None None None]
This paper provides a minor improvement paper of DeepRS. The major improvement comes from the coupling of user-item factors in prediction. While the motivation is clear the improvement of the model architecture is minor. I think the author should improve the paper to discuss more on the impact of introduction of coupling which might make this paper stronger. Specifically conduct isolate experiment to change loss architecture gradually from a non-coupled network to a final proposed coupled network to demonstrate the importance of coupling.Another important missing part of the paper seems to be time complexity. Since coupled net would be much more costly to generate recommendations a discussion on how it would impact real world usages should be added.Overall I think this is a paper that should be improved before accepted.,Collaborative Deep Embedding via Dual Networks,2.5837000000000003,False,[None None None None None]
The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled the authors responded that subsampling is common in recommender systems work including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea and in particular doesn't answer the question of "how would the results look without subsampling" which I think is a question that could easily have been answered directly.Especially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper in seems odd to sample the data to reduce sparsity.Other than that the pre-review questions seem to have been answered satisfactorily.The contribution of the paper is to propose user and item embedding methods as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS though the network formulation has some differences.Overall this is an reasonably put together paper that makes a contribution in an important area though there are still some shortcomings that should be addressed namely:1) The evaluation is unusual. Recall@M is the only result reported though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness even if the results are not strong2) Given that the contribution is fairly simple (i.e. the "standard" recommender systems task but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used and exactly the same error measure for the fairest comparison possible.Without the above it's hard to tell how much the performance improvements are really due to the method being better versus the choice of datasets and the choice of loss functions.,Collaborative Deep Embedding via Dual Networks,-0.5597000000000001,False,[None None None None None]
This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that in contrast to most related papers work in the space of affine transformations (instead of pixels or flow). Said another way the network takes as input a set of affine transforms that describe the motion of patches in the past frames and likewise outputs a set of affine transforms that predict future patch motion.To that aim the authors make a few simplifying hypotheses namely that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses i.e. model the flow as a smoothly varying affine field (for instance see "Locally affine sparse-to-dense matching for motion and occlusion estimation" by Leordeanu et al. "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow" by Revaud et al. "Optical Flow With Semantic Segmentation and Localized Layers" by Sevilla-Lara et al.). These methods are state of the art which gives a hint about the validity of this kind of approach. In addition it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed the (patch-affine) motion space is considerably smaller than the image space making the problem much more tractable and amenable to high-resolution videos.While I agree with the authors on these points I also find that the paper suffer from important flaws. Specifically:  - the choice of not comparing with previous approaches in term of pixel prediction error seems very "convenient" to say the least. While it is clear that the evaluation metric is imperfect it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on e.g. the moving digits datasets (Figure 4) looks ok and can definitely be compared with other papers. Yet the authors chose not to which is suspicious.    - The newly proposed metric poses several problems. First action classification is evaluated with C3D which is not a state-of-the-art approach at all for this task. Second this metric actually *does not* evaluate what the network is claimed to do that is next frame prediction. Instead it evaluates if another network which was never trained to distinguish between real or synthetic frames by the way can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition it does not really make sense to train a network for something else that the final task it is evaluated for.    - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow which is still an active subject of research. Therefore an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input I find it important to discuss this aspect. How do motion estimation failures impact the network? Also the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.    - Even ignoring the weird proxy-evaluation part the network is still not trained end-to-end. That is the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results but other types of losses do exist for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves minimizing a loss in the transformation space where affine parameters are harder to intepret introduces unexpected artifacts. The motion is often largely underestimated as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames.     - The proposed approach is not sufficiently compared to previous work. In particular the approach is closely related to "SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY" of Taraucean et al ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.  - The comparison with optical flow is unfair. First the approach of Brox et al. is more than 10 years old. Second it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall the approach is not compared to very challenging baselines.  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0 X_1 ...} and predicted frames as {Y_1 Y_2 ...}. When asked if the videos at,Transformation-based Models of Video Sequences,-4.2444,False,['male' 'female' None 'male' 'male' None]
There were serious concerns raised about the originality of the work in regard to prior methods. The authors' responses to numerous reviewer questions on this matter were also unsatisfactory. In the end it is difficult to tell what the contribution in regard to the video prediction model is. The proposed evaluation metric is interesting but also raised serious concerns. Finally several reviewers raised concerns about the quality of the results.,Transformation-based Models of Video Sequences,-0.3034,False,['male' 'female' None 'male' 'male' None]
The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.Many previous works have proposed similar methods. The authors in their responses state that previous work is deterministic yet the proposed model also does not handle multimodality.Further i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output to be able to quantify the importance of using transformations both as input and output since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying "if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results" that is misunderstanding what the suggestion was. So currently it does not seem to be a valid novel contribution in this work compared to previous works.,Transformation-based Models of Video Sequences,0.1381,False,['male' 'female' None 'male' 'male' None]
Paper SummaryThis paper makes two contributions -(1) A model for next step prediction where the inputs and outputs are in thespace of affine transforms between adjacent frames.(2) An evaluation method in which the quality of the generated data is assessedby measuring the reduction in performance of another model (such as aclassifier) when tested on the generated data.The authors show that according to this metric the proposed model works betterthan other baseline models (including the recent work of Mathieu et al. whichuses adversarial training).Strengths- This paper attempts to solve a major problem in unsupervised learning  with videos which is evaluating them.- The results show that using MSE in transform space does prevent the blurring  problem to a large extent (which is one of the main aims of this paper).- The results show that the generated data reduces the performance of the C3D  model on UCF-101 to a much less extent than other baselines.- The paper validates the assumption that videos can be approximated to quite a  few time steps by a sequence of affine transforms starting from an initialframe.Weaknesses- The proposed metric makes sense only if we truly just care about the performance  of a particular classifier on a given task. This significantly narrows thescope of applicability of this metric because arguably one the importantreasons for doing unsupervised learning is to come up a representation that iswidely applicable across a variety of tasks. The proposed metric would not helpevaluate generative models designed to achieve this objective.- It is possible that one of the generative models being compared will interact  with the idiosyncrasies of the chosen classifier in unintended ways.Therefore it would be hard to draw strong conclusions about the relativemerits of generative models from the results of such experiments. One way toameliorate this would be to use several different classifiers (C3Ddual-stream network other state-of-the-art methods) and show that the rankingof different generative models is consistent across the choice of classifier.Adding such experiments would help increase certainty in the conclusions drawnin this paper.- Using only 4 or 8 input frames sampled at 25fps seems like very little context  if we really expect the model to extrapolate the kind of motion seen inUCF-101. The idea of working in the space of affine transforms would be muchmore appealing if the model can be shown to really generated non-trivial motionpatterns. Currently the motion patterns seem to be almost linearextrapolations.- The model that predicts motion does not have access to content at all. It only  gets access to previous motion. It seems that this might be a disadvantagebecause the motion predictor cannot use any cues like object boundaries ordecide what to do when two motion fields collide (it is probably easier to argueabout occlusions in content space).Quality/ClarityThe paper is clearly written and easy to follow. The assumptions are clearlyspecified and validated. Experimental details seem adequate.OriginalityThe idea of generating videos by predicting motion has been used previously.Several recent papers also use this idea. However the exact implementation inthis paper is new. The proposed evaluation protocol is novel.SignificanceThe proposed evaluation method is an interesting alternative especially if itis extended to include multiple classifiers representative of differentstate-of-the-art approaches. Given how hard it is to evaluate generative modelsof videos this paper could help start an effort to standardize on a benchmarkset.Minor comments and suggestions(1) In the caption for Table 1: ``Each column shows the accuracy on the test setwhen taking a different number of input frames as input" - ``input" here refersto the input to the classifier (Output of the next step prediction model). Howeverin the next sentence ``Our approach maps 16 \times 16 patches into 8 \times 8with stride 4 and it takes 4 frames at the input" - here ``input" refers tothe input to the next step prediction model. It might be a good idea to rephrasethese sentences to make the distinction clear.(2) In order to better understand the space of affine transformparameters it might help to include a histogram of these parameters in thepaper. This can help us see at a glance what is the typical range of these6 parameters should we expect a lot of outliers etc.(3) In order to compare transforms A and B instead of ||A - B||^2 onecould consider A^{-1}B being close to identity as the metric. Did the authorstry this ?(4) "The performance of the classifier on ground truth data is an upper bound onthe performance of any generative model." This is not *strictly* true. It ispossible (though highly unlikely) that a generative model might make the datalook cleaner sharper or highlight some aspect of it which could improve theperformance of the classifier (even compared to ground truth). This isespecially true if the the generative model had access to the classifier itcould then see what makes the classifier fire and highlight those discriminativefeatures in the generated output.OverallThis paper proposes future prediction in affine transform space. This doesreduce blurriness and makes the videos look relatively realistic (at least to theC3D classifier). However the paper can be improved by showing that the model canpredict more non-trivial motion flows and the experiments can be strengthened byadding more classifiers besides than C3D.,Transformation-based Models of Video Sequences,8.656500000000001,False,['male' 'female' None 'male' 'male' None]
This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that in contrast to most related papers work in the space of affine transformations (instead of pixels or flow). Said another way the network takes as input a set of affine transforms that describe the motion of patches in the past frames and likewise outputs a set of affine transforms that predict future patch motion.To that aim the authors make a few simplifying hypotheses namely that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses i.e. model the flow as a smoothly varying affine field (for instance see "Locally affine sparse-to-dense matching for motion and occlusion estimation" by Leordeanu et al. "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow" by Revaud et al. "Optical Flow With Semantic Segmentation and Localized Layers" by Sevilla-Lara et al.). These methods are state of the art which gives a hint about the validity of this kind of approach. In addition it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed the (patch-affine) motion space is considerably smaller than the image space making the problem much more tractable and amenable to high-resolution videos.While I agree with the authors on these points I also find that the paper suffer from important flaws. Specifically:  - the choice of not comparing with previous approaches in term of pixel prediction error seems very "convenient" to say the least. While it is clear that the evaluation metric is imperfect it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on e.g. the moving digits datasets (Figure 4) looks ok and can definitely be compared with other papers. Yet the authors chose not to which is suspicious.    - The newly proposed metric poses several problems. First action classification is evaluated with C3D which is not a state-of-the-art approach at all for this task. Second this metric actually *does not* evaluate what the network is claimed to do that is next frame prediction. Instead it evaluates if another network which was never trained to distinguish between real or synthetic frames by the way can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition it does not really make sense to train a network for something else that the final task it is evaluated for.    - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow which is still an active subject of research. Therefore an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input I find it important to discuss this aspect. How do motion estimation failures impact the network? Also the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.    - Even ignoring the weird proxy-evaluation part the network is still not trained end-to-end. That is the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results but other types of losses do exist for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves minimizing a loss in the transformation space where affine parameters are harder to intepret introduces unexpected artifacts. The motion is often largely underestimated as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames.     - The proposed approach is not sufficiently compared to previous work. In particular the approach is closely related to "SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY" of Taraucean et al ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.  - The comparison with optical flow is unfair. First the approach of Brox et al. is more than 10 years old. Second it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall the approach is not compared to very challenging baselines.  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0 X_1 ...} and predicted frames as {Y_1 Y_2 ...}. When asked if the videos at ,Transformation-based Models of Video Sequences,-4.2444,False,['male' 'female' None 'male' 'male' None]
Could the authors comment on the relationship between their work and these previous works that appear to use a similar transformation-based video prediction technique?Dynamic Filter Networks (NIPS 2016)Unsupervised Learning for Physical Interaction through Video Prediction (NIPS 2016)Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks (NIPS 2016),Transformation-based Models of Video Sequences,0.5719,False,['male' 'female' None 'male' 'male' None]
The transformation-based approach for generating the next frame in a sequence was used in ICLRw2016 ,Transformation-based Models of Video Sequences,0.0,False,['male' 'female' None 'male' 'male' None]
SUMMARY.The paper propose a new scoring function for knowledge base embedding.The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.The proposed function is tested on two tasks knowledge-base completion and question answering.----------OVERALL JUDGMENTWhile I think this proposed work is very interesting and it is an idea worth to explore further the presentation and the experimental section of the paper have some problems.Regarding the presentation as far as I understand this is not an attention model as intended standardly in the literature.Plus it has hardly anything to share with memory networks/neural Turing machines the parallel that the authors try to make is not very convincing.Regarding the experimental section for a fair comparison the authors should test their model on standard benchmarks reporting state-of-the-art models.Finally the paper lack of discussion of results and insights on the behavior of the proposed model.----------DETAILED COMMENTSIn section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so does it make any sense?,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,0.7160000000000001,False,['female' 'male' 'male']
Three knowledgable reviewers recommend rejection. While they agree that the paper has interesting aspects they suggest a more convincing evaluation. The authors did not address some of the reviewer's concerns. The AC strongly encourages the authors to improve their paper and resubmit it to a future conference.,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,1.3361,False,['female' 'male' 'male']
The contribution of this paper can be summarized as:1 A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al 2015).2. Along with the entity / relation representations trained by TransGaussian an LSTM + attention model is built on natural language questions aiming at learning a distribution (not normalized though) over relations for question answering.3. Experiments on a generated WorldCup2014 dataset focusing on path queries and conjunctive queries.Overall I think the Gaussian parameterization exhibits some nice properties and could be suitable to KB completion and question answering. However some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:[Major comments]- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering there are many existing and competitive benchmarks (e.g. FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover the questions are just generated by a few templates which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. - Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct so it is more necessary to justify this on real QA datasets.- The model is named as  “Gaussian attention” and I kind of think it is not very closely related to well-known attention mechanism but more related to KB embedding literature.[Minor comments]- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations and the second row of those denote every single word of the NL question. Maybe make it clearer?- Besides “entity recognition” usually we still need an “entity linker” component which links the text mention to the KB entity. ,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,0.8884000000000003,False,['female' 'male' 'male']
This paper presents extensions to previous work using embeddings for modeling Knowledge Bases and performing Q&A on them centered around the use of multivariate gaussian likelihood instead of inner products to score attention. This is supposed to allow more control on the attention by dealing with its spread.This is a dense paper centered around a quite complicated model. With the supplementary material this makes a 16p paper. It might be clearer to make 2 separate papers: one on KB completion and another one on Q&A.I like the idea of controlling the spread of the attention. This makes sense. However I do not feel that this paper is convincing enough to justify its use compared to usual inner products.For several reasons:- These should be more ablation experiments to separate the different pieces of the model and study their influence separately. The only interesting point in that sense is Table 8 in Appendix B. We need more of this. - In particular a canonical experiments comparing Gaussian interaction vs inner product would be very useful. - Experiments on existing benchmarks (for KB completion or QA) would help. I agree with the authors that it is difficult to find the perfect benchmark so it is a good idea to propose a new one (WorldCup2014). But this should come in addition to experiments on existing data.- Table 11 of Appendix C (page 16) that compares TransE and TransGaussian for the task of link prediction on WordNet can be seen as fixing the two points above (simple setting on existing benchmark). Unfortunately TransGaussian does not perform well compared to simpler TransE. This along with the poor results of TransGaussian (SINGLE) of Table 2 indicate that training TransGaussian seems pretty complex and hence question the actual validity of this architecture.,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,2.5976999999999997,False,['female' 'male' 'male']
SUMMARY.The paper propose a new scoring function for knowledge base embedding.The scoring function called TransGaussian is an novel take on (or a generalization of) the well-known TransE scoring function.The proposed function is tested on two tasks knowledge-base completion and question answering.----------OVERALL JUDGMENTWhile I think this proposed work is very interesting and it is an idea worth to explore further the presentation and the experimental section of the paper have some problems.Regarding the presentation as far as I understand this is not an attention model as intended standardly in the literature.Plus it has hardly anything to share with memory networks/neural Turing machines the parallel that the authors try to make is not very convincing.Regarding the experimental section for a fair comparison the authors should test their model on standard benchmarks reporting state-of-the-art models.Finally the paper lack of discussion of results and insights on the behavior of the proposed model.----------DETAILED COMMENTSIn section 2.2 when the authors calculate \mu_{context} do not they loose the order of relations? And if it is so does it make any sense?,Gaussian Attention Model and Its Application to Knowledge Base Embedding and Question Answering,0.7160000000000001,False,['female' 'male' 'male']
After rebuttal:Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself and as the authors mention it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover some claims in the paper look wrong in the light of these results for example:- "This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture."- "Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification."These statements and possibly other parts of the paper have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.--------Initial review:The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect fairly convincing inpaintings are produced.The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete see below). Overall I am in the borderline mode and I will gladly raise the score if the authors address my concerns regarding the experiments.1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper but I do not understand why the authors are not willing to simply run their method with AlexNet architecture although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results but it is also very important to properly compare to prior work. On a related topic Doersch et al. also tried using VGG architecture would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance [Pathak et al 2016]) is missing and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at but they do not add as much to the paper as they could.,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,3.2568000000000006,False,['female' 'male' 'male']
There has been prior work on semi-supervised GAN though this paper is the first context conditional variant. The novelty of the approach was questioned by two of the reviewers as the approach seems more incremental. Furthermore it would have been helpful if the issues one of the reviewer had with statements in the document were addressed.,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,0.3188,False,['female' 'male' 'male']
This paper presents a semi-supervised algorithm for regularizing deep convolutional neural networks.They propose an adversarial approach for image inpainting where the discriminator learns to identify whether an inpainted image comes from the data distribution or the generator while at the same time it learns to recognize objects in an image from the data distribution. In experiments they show the usefulness of their algorithm in which the features learned by the discriminator result in comparable or better object recognition performance to the reported state-of-the-art in two datasets.Overall the proposed idea seems a simple yet an effective way for regularize CNNs to improve the classification performance. ,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,0.9821,False,['female' 'male' 'male']
This paper proposes a method to incorporate super-resolution and inpainting in the GAN framework for semi-supervised learning using the GAN discriminative features on larger images.The core idea of the paper is not very novel. The usefulness of the GAN discriminative features for semi-supervised learning is already established in previous works such as CatGAN DCGAN and Salimans et al. However this paper does a good job in actually getting the semi-supervised GAN framework working on larger images such as STL-10 and Pascal datasets using the proposed context conditioning approach and achieves the state-of-the-art on these datasets.I think that the authors should provide the SSL-GAN baseline for the PASCAL dataset as it is very important to compare the contribution of the context conditioning idea with the standard way of using GAN for semi-supervised learning i.e. SSL-GAN. I can't see why the SSL-GAN can not be applied to the 64*64 and 96*96 version of the Pascal dataset (Table 3). If they have trouble training the vanilla GAN on Pascal even on the 64*64 image size this should be mentioned in the paper and be explained. I am concerned about this specially because CC-GAN almost matches the SSL-GAN baseline on STL-10 and CC-GAN2 to me seems like a hacky way to improve upon the core CC-GAN idea. So it would be great to compare CC-GAN and SSL-GAN on some other dataset even if it is a downsampled PASCAL dataset.,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,1.5995,False,['female' 'male' 'male']
After rebuttal:Thanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself and as the authors mention it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover some claims in the paper look wrong in the light of these results for example:- "This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture."- "Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification."These statements and possibly other parts of the paper have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.--------Initial review:The paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect fairly convincing inpaintings are produced.The proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete see below). Overall I am in the borderline mode and I will gladly raise the score if the authors address my concerns regarding the experiments.1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper but I do not understand why the authors are not willing to simply run their method with AlexNet architecture although two commenters asked them to do so. Such an experiment would strongly support authors’ claims. Current reasoning that “we thought it reasonable to use more current models while making the difference clear” is not convincing. It is great that better architectures lead to better results but it is also very important to properly compare to prior work. On a related topic Doersch et al. also tried using VGG architecture would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance [Pathak et al 2016]) is missing and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at but they do not add as much to the paper as they could.,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,3.2568000000000006,False,['female' 'male' 'male']
Nice work! I am curious about the SSL experiments: since ,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,0.7935,False,['female' 'male' 'male']
The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks the method aims to identify individual task and solve them separately. To this end the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence i.e. a task only changes representation between training episodes. The method has been evaluated on two tasks multi-task slot-car racing and mobile navigation to prove its efficacy.there were several unclear issues:1. The first question is that if the method is only appealing on the scenario like the slot-car racing otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting in particular over the multi-task learning?The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work without such comparison the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.2.Following up to the previous question please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method in particular compared to the multi-slot car-racing driving experiment there is too little results to make sound argument (almost no comparison to alternative methods i.e. no baseline method is that true for the problem).The explanation of the authors did provide more details and more explicit information. 3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong) a possible baseline would be a hard gated unit how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.The author simply stated again what they did for the modeling without counter the comparison to hard-gating but it's probably less an issue compared to Question 1.In summary while there are remaining concerns about lacking comparisons the is a weak tendency towards accepting the submission.,Unsupervised Learning of State Representations for Multiple Tasks,0.04900000000000003,False,['male' 'male' 'male' 'male' 'male']
The authors propose to explore an important problem -- learning state representations for reinforcement learning. However the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end there is insufficient evidence to convincingly show that the method actually works and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices) the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks.,Unsupervised Learning of State Representations for Multiple Tasks,0.694,False,['male' 'male' 'male' 'male' 'male']
We would like to thank all reviewers for their thorough and helpful comments!1) Before we turn to the individual questions raised the reviewers we would like to address the main issue that all reviewers raised namely the relationship of our method to multi-task learning:“The authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.” (AnonReviewer1)“The argument did not support the lack of comparison to multi-task joint-learning.” (AnonReviewer2)“Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks” (AnonReviewer 3)The intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. But there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance in a video game an agent have to achieve several subgoals (fight an enemy avoid obstacles...) each of these can be seen as individual task. Learning multiple (sub-)policies dedicated to *different* tasks is a problem of its own right as it faces significant theoretical issues such as “catastrophic forgetting”. We have elaborated on this argument in the introduction.Since there is few work approaching this problem in RL our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. We now reply to the individual comments raised by the reviewers.----AnonReviewer11) “References to other multi-task learning works e.g. policy distillation and actor-mimic (both ICLR ’16) may be appropriate as well.”Thank you for the pointers we have integrated the two suggested papers in the related work of the paper.2) “The approach does not necessarily share information across tasks for better learning and requires learning a different policy for each task”This is the very idea of the method proposed we updated the introduction to clarify the reasons we focused on this approach.3) “In the second navigation scenario only the state representation is qualitatively shown not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better the paper should also evaluate on control with the same comparisons as in the first experiment. Without this evaluation the experiment is incomplete.”We agree and as mentioned before this was a preliminary and incomplete experiment and we decided to remove it from the paper. 4) “Lastly to be on par with publications at a venue like ICLR the method should be evaluated more thoroughly on a wider range of set-ups [...]”We agree that it is beneficial to apply a method to a wider range of tasks. Yet we chose to invest into rigorously evaluating the performance of the method on the chosen task and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks but we will have to address this in future work.5) “Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.”Yes in principle it would be possible to use other state representation learning objectives.  Note however that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP as it has will try to explain all variations in the observation in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.6) ”One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car) as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).”Thank you for this suggestion; in our experiment however the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks and in consequence a separate policy for each slot car is learned.7) “Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.”Our experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data but for now even in our largest experiments we did not see it happening.8) “If there are aliasing issues with the images why not just use higher resolution images?”Mainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm yet we did not have the computational power to do this exhaustively on higher resolutions.---AnonReviewer21) “The author simply stated again what they did for the modeling without counter the comparison to hard-gating but it's probably less an issue compared to Question 1.”We are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable we cannot train it using backpropagation).---AnonReviewer32) “Parameters choice is arbitrary (w parameters)”The weights w for the different methods are chosen as described in Jonschkowski & Brock 2015 by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.3) “The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.”We agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out though that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work too e.g. (Lange et al. 2012). Moreover it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).But we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.,Unsupervised Learning of State Representations for Multiple Tasks,7.1197,False,['male' 'male' 'male' 'male' 'male']
This paper is about learning unsupervised state representations using multi-task reinforcement learning.  The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound.Positives:+ Gating to enable learning a joint representation+ Multi-task learning extended from a single task in prior work+ Combining multiple types of losses to learn a strong representation (Coherence Proportionality Causality Repeatability Consistency and Separation)Negatives:- Parameters choice is arbitrary (w parameters)- Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks- The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.,Unsupervised Learning of State Representations for Multiple Tasks,2.349,False,['male' 'male' 'male' 'male' 'male']
This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks rather than a single task. The research direction of learning representations for multiple tasks is an interesting one and largely unexplored. The approach in the paper is to learn a different representation for each task and a different policy for each task where the task is detected automatically and built into the neural network.The authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper as discussed in the pre-review question phase. References to other multi-task learning works e.g. policy distillation and actor-mimic (both ICLR ’16) may be appropriate as well.The method proposes to jointly learn a task classifier with a state representation learner by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard non-multitask approach.The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task detecting the “task” is very easy and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.In the second navigation scenario only the state representation is qualitatively shown not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better the paper should also evaluate on control with the same comparisons as in the first experiment. Without this evaluation the experiment is incomplete.Lastly to be on par with publications at a venue like ICLR the method should be evaluated more thoroughly on a wider range of set-ups to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory the method should scale the experiments do not demonstrate that it can handle more realistic scenarios such as scaling beyond MNIST-level images to 3D or real images or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important because unexpected issues can come up when trying to scale. If scaling-up is straight-forward then running this experiment (and including it in the paper) should be straight-forward.In summary here are the pros and cons of this paper:Cons- The approach does not necessarily share information across tasks for better learning and requires learning a different policy for each task- Only one experimental set-up that evaluates learned policy with multi-task state representation- No experiments on more realistic scenarios such 3D environments or high-dimensional control problemsPros: - This approach enables using the same network for multiple tasks which is often not true for transfer and multi-task learning approaches- Novel way to learn a single policy for multiple tasks including a task coherence prior which ensures that the task classification is meaningful- Experimentally validated on two toy tasks. One task shows improvement over baseline approachesThus my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.Lastly here are some minor comments/questions on how I think the paper could be improved but are not as important as the above:Approach:Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.Experiments:One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car) as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right).Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.If there are aliasing issues with the images why not just use higher resolution images?,Unsupervised Learning of State Representations for Multiple Tasks,7.165700000000002,False,['male' 'male' 'male' 'male' 'male']
This work investigates the performance of transfer learning from resource-rich setup (BookTest CNN/Daily Mail corpora) to low-resource (bAbI SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However when the model is exposed to few training instances some improvements are observed.The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [12].More importantly the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3] these claims bring a marginal novelty and still "how and why" should be central in this work.  [1],Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,0.9334,False,['male' 'male' 'male' 'male']
The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,0.6238,False,['male' 'male' 'male' 'male']
Dear authors and reviewers this paper is currently very close to the decision boundary for acceptance and would benefit from a bit more discussion.,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,0.8225,False,['male' 'male' 'male' 'male']
First I would like to apologize for the delay in reviewing.summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader) in an artificial and well populated dataset in order to perform in another target dataset. Here is what I understand are their several experiments to transfer learning but I am not 100% sure.1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1)2. The model is pre-trained on the big artificial dataset like before then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2).3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder  component. Alternatively each component is reset to a random initialization to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3)I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks and sometimes what is reported is the mean performance across the tasks sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ?Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1 when the nuanced difference between the datasets are exposed.Minor: unexplained acronyms: GRU BT CBT.benfits p. 2subsubset p. 6,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,3.0382,False,['male' 'male' 'male' 'male']
This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant.This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance.Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However there is not much of that. The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI Gutenberg books and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets?Unfortunately there is not much to take-away from this paper. ,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,-1.7514999999999998,False,['male' 'male' 'male' 'male']
This work investigates the performance of transfer learning from resource-rich setup (BookTest CNN/Daily Mail corpora) to low-resource (bAbI SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However when the model is exposed to few training instances some improvements are observed.The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [12].More importantly the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3] these claims bring a marginal novelty and still "how and why" should be central in this work.  [1] ,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,0.9334,False,['male' 'male' 'male' 'male']
Dear AuthorsPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!,Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension,1.0193,False,['male' 'male' 'male' 'male']
The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.The authors put in considerable effort to explain all details of the paper clearly and at length so the content of the paper is accessible even to people novel to pruning methods. Additionally the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.My major criticism is that the paper lacks focus does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent I here summarise each paragraph of the conclusion section:Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky but further investigation neededParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baselineParagraph 3: Re-training may help but is not fairParagraph 4: Brute-force can prune 40-70% in shallow networksParagraph 5: Brute-force less effective in deep networksParagraph 6: Not all neurons contribute equally to performance of networkThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion and no sentence in the part on results in the abstract even refers to neural representations. In an answer to the pre-review questions the authors stated:> Furthermore we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be > pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be > impossible if neurons did not belong to the distinct classes we describe."But this can already be concluded from the 2nd order method which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?In addition some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally one would need to consider all the different ways of pruning (which of course scales exponentially with the number of neurons and is computationally infeasible). Notably the authors do consider this limitation in the context of conventional pruning methods in the conclusions: "Third we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process". But the brute-force pruning process is also serial - why is that not a problem?All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.PS: I think the confusion starts with the following sentence in the abstract: "In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning." Both aspects are pretty orthogonal but are completely mixed up in the paper.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,1.5811000000000002,False,['male' 'male' 'male' None]
The paper does not seem to have enough novelty and the contribution is not clear enough due to presentation issues.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,-0.2924,False,['male' 'male' 'male' None]
1) Wen Wei et al. "Learning structured sparsity in deep neural networks." Advances in Neural Information Processing Systems. 2016.2) Lebedev Vadim and Victor Lempitsky. "Fast convnets using group-wise brain damage." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.3) Alvarez Jose M. and Mathieu Salzmann. "Learning the Number of Neurons in Deep Networks." Advances in Neural Information Processing Systems. 2016.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,0.3272,False,['male' 'male' 'male' None]
The authors have put forward a sincere effort to investigate the "fundamental nature of learning representations in neural networks" a topic of great interest and importance to our field.  They propose to do this via a few simplistic pruning algorithms to essentially monitor performance decay as a function of unit pruning.  This is an interesting idea and one that could potentially be instructive though in total I don't think that has been achieved here.  First I find the introduction of pruning lengthy and not particularly novel or surprising.  For example Fig 1 is not necessary nor is most of the preamble section 3.3.0.  The pruning algorithms themselves are sensible (though overly simplistic) approaches which of course would not matter if they were effective in addressing the question.  However in looking for contributions this paper makes an interesting pithy or novel take on pruning is not one of them in my opinion.Second and most relevant to my overall rating Section 4 does not get deeper than scratching the surface.  The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value.  The experiments themselves are not particularly deep covering a toy problem and MNIST which does not convince me that I can draw lessons to the broader story of neural networks more generally.  Third there is no essential algorithmic architectural or mathematical insight which I expect out of all but the most heavily experimental papers.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,1.8204,False,['male' 'male' 'male' None]
I did enjoy reading some of the introductions and background in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method.However there are many reasons why I think that this work is not appropriate for ICLR. For one there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore there are many statements in the text that are not necessarily true in particular in light of deep networks with modern regularization methods. For example the authors state that the most accurate method is what they call brute-force. However this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough and the points should be included in the paper.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,-0.9919999999999999,False,['male' 'male' 'male' None]
Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.Q: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?A: Under certain motivational assumptions it is understandable to demand benchmarking comparisons against state-of-the-art methods but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most if not all popoular pruning techniques out there. In fact we examined the literature for numerical methods to approximate the importance of network elements and the widely-cited 1st & 2nd order techniques proposed by Mozer LeCun Hassibi Stork et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.Q: The idea of using Taylor series approximations seems interesting but not really effective.A: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.Q: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? A: As mentioned above the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative unless we use the soft-plus function as a continuous approximation. Furthermore due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola if there is no useful parabola to approximate this relationship then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn’t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. Q: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?A: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5 which specifically examines non-optimally trained networks) so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset task activation function or network architecture will not change the trends we see in the results but could make the results less interpretable. Q: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.A: The brute-force method is highly parallelizable so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept and we intend to investigate potential speedups in future work. Also since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time) this is potentially acceptable. ,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,5.090599999999999,False,['male' 'male' 'male' None]
First Revision: 27 November 2016. Added Section 4.5: Investigation of Pruning Performance with Imperfect Starting Conditions. We discuss the impact of pruning sub-optimally trained networks in this section by specifically analyzing performance of networks classifying the digits 0 1 and 2 of the MNIST database.,The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning,-0.3182,False,['male' 'male' 'male' None]
The paper presents a new environment called Retro Learning Environment (RLE) for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games and some results using a new "rivalry metric".These environments (or more generally standardized evaluation methods like public data sets competitions etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.That said the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And yes domain knowledge helps but this is obvious. The rivalry training is an interesting idea when training against a different opponent the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly it gets evaluated on how well it does against the in-game AI! Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished so this is disappointing. In the end I'm not very excited about this paper.I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example mloss.org could be a better 'venue' JMLR has an associated journal track for accompanying papers:,Playing SNES in the Retro Learning Environment,5.145,False,['male' 'male' 'male']
The authors present a new set of environments similar to ALE but based on Super Nintendo rather than Atari. This is a great asset and could be important for RL research but it doesn't merit ICLR publication because of the lack of novel research ideas. Hopefully the authors will consider another venue to publish this paper such as perhaps a journal or workshop.,Playing SNES in the Retro Learning Environment,1.3271,False,['male' 'male' 'male']
The paper presents a new environment called Retro Learning Environment (RLE) for reinforcement learning. The authors focus on Super Nintendo but claim that the interface supports many others (including ALE). Benchmark results are given for standard algorithms in 5 new Super Nintendo games and some results using a new "rivalry metric".These environments (or more generally standardized evaluation methods like public data sets competitions etc.) have a long history of improving the quality of AI and machine learning research. One example in the past few years was the Atari Learning Environment (ALE) which has now turned into a standard benchmark for comparison of algorithms and results. In this sense the RLE could be a worthy contribution to the field by encouraging new challenging domains for research.That said the main focus of this paper is presenting this new framework and showcasing the importance of new challenging domains. The results of experiments themselves are for existing algorithms. There are some new results that show reward shaping and policy shaping (having a bias toward going right in Super Mario) help during learning. And yes domain knowledge helps but this is obvious. The rivalry training is an interesting idea when training against a different opponent the learner overfits to that opponent and forgets to play against the in-game AI; but then oddly it gets evaluated on how well it does against the in-game AI! Also the part of the paper that describes the scientific results (especially the rivalry training) is less polished so this is disappointing. In the end I'm not very excited about this paper.I was hoping for a more significant scientific contribution to accompany in this new environment. It's not clear if this is necessary for publication but also it's not clear that ICLR is the right venue for this work due to the contribution being mainly about the new code (for example mloss.org could be a better 'venue' JMLR has an associated journal track for accompanying papers: ,Playing SNES in the Retro Learning Environment,5.145,False,['male' 'male' 'male']
This paper introduces a new reinforcement learning environment called « The Retro Learning Environment” that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment but more generic). The first supported platform is the SNES with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari’s (due to more complex graphics AI and game mechanics). Several DQN variants are evaluated in experiments and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.I like the idea of going toward more complex games than those found on Atari 2600 and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out though I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games especially some Nintendo still makes money from.Besides the introduction of the environment it is good to have DQN benchmarks on five games but this does not add a lot of value. The authors also mention as contribution "A new benchmarking technique allowing algorithms to compete against each other rather than playing against the in-game AI" but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.Overall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean however that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.Other small comments:- There are lots of typos (way too many to mention them all)- It is said that Infinite Mario "still serves as a benchmark platform" however as far as I know it had to be shutdown due to Nintendo not being too happy about it- "RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?- Why is there no DQN / DDDQN result on Super Mario?- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not- The Du et al reference seems incomplete,Playing SNES in the Retro Learning Environment,1.5985,False,['male' 'male' 'male']
This paper presents a valuable new collection of video game benchmarks in an extendable framework and establishes initial baselines on a few of them.Reward structures: for how many of the possible games have you implemented the means to extract scores and incremental reward structures? From the github repo it looks like about 10 -- do you plan to add more and when?“rivalry” training: this is one of the weaker components of the paper and it should probably be emphasised less. On this topic there is a vast body of (uncited) multi-agent literature it is a well-studied problem setup (more so than RL itself). To avoid controversy I would recommend not claiming any novel contribution on the topic (I don’t think that you really invented “a new method to train an agent by enabling it to train against several opponents” nor “a new benchmarking technique for agents evaluation by enabling them to compete against each other rather than playing against the in-game AI”). Instead just explain that you have established single-agent and multi-agent baselines for your new benchmark suite.Your definition of Q-function (“predicts the score at the end of the game given the current state and selected action”) is incorrect. It should read something like: it estimates the cumulative discounted reward that can be obtained from state s starting with action a (and then following a certain policy).Minor:* Eq (1): the Q-net inside the max() is the target network with different parameters theta’* the Du et al. reference is missing the year* some of the other references should point at the corresponding published papers instead of the arxiv versions,Playing SNES in the Retro Learning Environment,1.3653999999999997,False,['male' 'male' 'male']
This paper presents a principled optimization method for SGNS (word2vec).While the proposed method is elegant from a theoretical perspective I am not sure what the tangible benefits of this approach are. For example does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see "Improving Distributional Similarity with Lessons Learned from Word Embeddings" (Levy et al. 2015)). The theoretical connection to Riemannian optimization is nice though and it might be useful for understanding related methods in the future.,Riemannian Optimization for Skip-Gram Negative Sampling,2.5728,False,['male' 'male' 'male' 'male' 'male']
The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem and solved using manifold optimization techniques is sound. However this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications.,Riemannian Optimization for Skip-Gram Negative Sampling,1.1925,False,['male' 'male' 'male' 'male' 'male']
The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. The computational cost of the two approaches is not carefully discussed. For instance how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? ,Riemannian Optimization for Skip-Gram Negative Sampling,0.8387000000000002,False,['male' 'male' 'male' 'male' 'male']
Dear authorsThe authors' response clarified some of my confusion. But I still have the following question:-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps step 1 looks for a low-rank X (by Riemannian optimization) step 2 factorizes X into two matrices (W C). You are claiming that your model outperforms previous approaches that directly optimizes over (W C). But since the end result (the factors) is the same can the authors provide some intuition and justification why the proposed method works better?As far as I can see though parameterized differently the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3) but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. -- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3 and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.Overall I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective if more thorough comparison and discussion (as mentioned above) are provided. On the other hand my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.,Riemannian Optimization for Skip-Gram Negative Sampling,3.6421,False,['male' 'male' 'male' 'male' 'male']
This paper presents a principled optimization method for SGNS (word2vec).While the proposed method is elegant from a theoretical perspective I am not sure what the tangible benefits of this approach are. For example does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see "Improving Distributional Similarity with Lessons Learned from Word Embeddings" (Levy et al. 2015)). The theoretical connection to Riemannian optimization is nice though and it might be useful for understanding related methods in the future.,Riemannian Optimization for Skip-Gram Negative Sampling,2.5728,False,['male' 'male' 'male' 'male' 'male']
The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.,Classless Association using Neural Networks,0.5174000000000001,False,['male' 'male' 'male' 'male']
The paper explores neural-network learning on pairs of samples that are labeled as either similar or dissimilar. The proposed model appears to be different from standard siamese architectures but it is poorly motivated. The experimental evaluation of the proposed model is very limited.,Classless Association using Neural Networks,0.31800000000000006,False,['male' 'male' 'male' 'male']
We have updated our paper.  The changes are* We have improved the clarity and motivation of our model* We have evaluated our model to three more classless datasets (Rotated-90 MNIST Inverted MNIST and Random Rotation MNIST).* We have updated Figure 4 and 5 for showing some random output classification samples instead of the mean of all images.* We have added two more examples and demo as supplemental material ,Classless Association using Neural Networks,0.802,False,['male' 'male' 'male' 'male']
The paper presents an alternative way of supervising the training of neural network without explicitly using labels when only link/not-link information is available between pairs of examples. A pair of network is trained each of which is used to supervise the other one.The presentation of the paper is not very clear the writing can be improved.Some design choice are not explained: Why is the power function used in the E-step for approximating the distribution (section 2.1)? Why do the authors only consider a uniform distribution? I understand that using a different prior breaks the assumption that nothing is known about the classes. However I do not see a practical situations where the proposed setting/work would be useful.  Also there exist a large body of work in semi-supervised learning with co-training based on a similar idea. Overall I think this work should be clarified and improved to be a good fit for this venue.,Classless Association using Neural Networks,1.4357,False,['male' 'male' 'male' 'male']
The paper explores a new technique for classless association a milder unsupervised learning where we do not know the class labels exactly but we have a prior about the examples that belong to the same class. Authors proposed a two stream architecture with two neural networks as streams process examples from the same class simultaneously. Both streams rely on the target (pseudo classes or cluster indices) of each other and the outputs an intermediate representation z which is forced to match with a statistical distribution (uniform in their case). The model is trained with EM where the E step obtains the current statistical distribution given output vectors z and M step updates the weights of the architecture given z and pseudo-classes. Experimental results on re-organized MNIST exhibits better performance compared to classical clustering algorithms (in terms of association accuracy and purity). The authors further provide comparison against a supervised method where proposed architecture expectedly performs worse but with promising results.The basic motivation of the architecture apparently relies on unlabeled data and agreement of the same pseudo-labels generated by two streams. But the paper is hard to follow and the motivation for the proposed architecture itself is hidden in details. What is trying to be achieved by matching distributions and using the pseudo-targets of the each other? Perhaps the statistical distribution of the classes is assumed to be uniform but how will it extend to other priors or even the case where we do not assume that we know the prior? The current setup needs justifications. What would be very interesting is to see two examples having the same class but one from MNIST the other from Rotated-MNIST or Background-MNIST. Because it is hard to guess how different the examples in two streams. At the end I feel like the authors have found a very interesting approach for classless association which can be extended to lots of many-to-one problems. This is a good catch. I would like to see the idea in the future with some extensive experiments on large scale datasets and tasks. But the current version lacks the theoretical motivations and convincing experiments. I would definitely recommend this paper to be presented in ICLR workshop.Few more points:Typo: Figure1. second line in the caption "that" -> "than"Necessity of Equation 2 is not clearBatch size M is enormous compared to classical models there is no explanation for thisWhy uniform? should be clarified (of course it is the simplest prior to pick but just a few words about it would be good for completeness)Typo: Page 6 second paragraph line 3: "that" -> "than",Classless Association using Neural Networks,4.2700000000000005,False,['male' 'male' 'male' 'male']
The paper looks correct but still i am not convinced about the experimentation performed. Perhaps another experiment with more challenging data would be welcome. Honestly i don't find a clear motivation for this work however it could have some potential and it would be interested to be presented in conference.,Classless Association using Neural Networks,0.5174000000000001,False,['male' 'male' 'male' 'male']
The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.However it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement especially literature review and experiment analysis.,Efficient Calculation of Polynomial Features on Sparse Matrices,0.8107,False,['male' 'male']
The approach/problem seems interesting and several reviewers commented on this. However the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers and putting these together at this time it makes sense to reject the paper.,Efficient Calculation of Polynomial Features on Sparse Matrices,0.49389999999999995,False,['male' 'male']
The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.Overall the algorithm is definitely interesting quite simple and nice with many possible applications. The paper is however very superficial in terms of experiments or applications of the proposed scheme. Most importantly the fit with the main scope of ICLR is far from obvious with this work that should probably re-submitted to better targets. ,Efficient Calculation of Polynomial Features on Sparse Matrices,2.9762,False,['male' 'male']
This paper proposes an algorithm for polynomial feature expansion on CSR matrices which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited) which are from decades ago. Many more relevant papers should be cited from the recent literature.The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k) which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments when d=1 there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as "likely a language implementation" which is not convincing. To fairly compare the two methods of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion there is no empirical experiments to show the advantage of the proposed method.Some minor problems are listed below.1) In Section 2 the notation "p_i:p_i+1" is not clearly defined.2) In Section 3.1 typo: "efter" - "after"3) All the algorithms in this paper are not titled. The input and output is not clearly listed.4) In Figure 1 the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.,Efficient Calculation of Polynomial Features on Sparse Matrices,-2.572,False,['male' 'male']
The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.However it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement especially literature review and experiment analysis. ,Efficient Calculation of Polynomial Features on Sparse Matrices,0.8107,False,['male' 'male']
This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.Comments1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.2) The proposed theory can only be applied to square loss setting (with linear update rule) making it somewhat limited. This paper would be much more interesting to ICLR community if the technique is applicable to general objective function and settings of deep neural networks.3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However the normal update of SGD might also benefit from SIMD especially when the dataset is dense.Overall even though the practical value of this work is limited by 2) and 3) the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community,Parallel Stochastic Gradient Descent with Sound Combiners,2.4968000000000004,False,['male' 'male' 'male' 'female']
The reviewers largely agree that this paper is well written and presents an interesting novel approach to parallelizing Stochastic Gradient Descent. However the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features although it is striking that very aggressive sketching (k~10) still works well. In this setting though there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.  The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney Petros Drineas and others) or broaden the scope of the proposed methods to include models of current interest (e.g. DNNs). The latter would of course be of specific interest to the ICLR community.,Parallel Stochastic Gradient Descent with Sound Combiners,1.6802000000000001,False,['male' 'male' 'male' 'female']
This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD by using a proposed sound combiner. To make such combiner more efficient the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.I feel that there might be some fundamental misunderstanding on SGD.''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w  and thus requires O(f)  space and time where f  is the number of features. In contrastM  is a f f  matrix and consequently the space and time complexity of parallel SGD is O(f^2) . In practice this would mean that we would need O(f) processors to see constant speedups an infeasible proposition particularly for datasets that can havethousands if not millions of features."I do not think one needs O(f^2) space and complexity for updating M_i * v where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper n should be much smaller than f. I seriously doubt that all author's assumptions experiments and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.I suggest authors to make the following changes to make this paper more clear and theoretically solid- provide computational complexity per step of the proposed algorithm- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.,Parallel Stochastic Gradient Descent with Sound Combiners,1.2106,False,['male' 'male' 'male' 'female']
Overall the idea in this paper is interesting and the paper is well-written and well-motivated.  However I think it is not ready to publish in ICLR for the following reasons:- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. - The proposed approach can only work for a small class of models and cannot apply to popular formulations  such as SVM logistic regression and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression the authors should compare their methods with linear programming approaches. Also it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). - The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse the proposed approach is only slightly better or is worse than Hogwild. For dense dataset it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together the experiment results are not convincing. ,Parallel Stochastic Gradient Descent with Sound Combiners,-0.4275,False,['male' 'male' 'male' 'female']
This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.Comments1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.2) The proposed theory can only be applied to square loss setting (with linear update rule) making it somewhat limited. This paper would be much more interesting to ICLR community if the technique is applicable to general objective function and settings of deep neural networks.3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However the normal update of SGD might also benefit from SIMD especially when the dataset is dense.Overall even though the practical value of this work is limited by 2) and 3) the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community,Parallel Stochastic Gradient Descent with Sound Combiners,2.4968000000000004,False,['male' 'male' 'male' 'female']
The authors agree with the reviewers that this manuscript is not yet ready.,Deep Convolutional Neural Network Design Patterns,0.1002,False,['female' None]
We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline,Deep Convolutional Neural Network Design Patterns,0.8141,False,['female' None]
The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100 but seem to achieve relatively poor performance on these datasets (Table 1) so their merit is unclear to me.I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years and such a summary could be useful for newcomers. However a lot of it seems to boil down to common sense (e.g. #1 #3 #7 #11). The rest of it might be more suited for an "introduction to training CNNs" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).The paper states that "it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer" which is wrong. We already discussed this previously re: my question about design pattern 5 but I think the answer that was given ("the nature of design patterns is that they only apply some of the time") does not excuse making such sweeping claims. This should probably be removed."We feel that normalization puts all the layer's input samples on more equal footing which allows backprop to train more effectively" (section 3.2 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with "we feel" as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object it is just a nonlinear function of x.Overall the paper reads like a collection of thoughts and ideas that are not very well delineated and the experimental results are unconvincing.,Deep Convolutional Neural Network Design Patterns,0.31529999999999997,False,['female' None]
The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example "Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality" is presented as one of 14 core design principles without any further justification. Similarly "Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference" is presented in the middle of a paragraph with no supporting references or further explanation.The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.Preliminary rating:It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.,Deep Convolutional Neural Network Design Patterns,5.3777,False,['female' None]
The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so a distributed representation of deep architectures. There are two aspects of the paper that I particularly valued: firstly the excellent review of recent works which made me realize how many things I have been missing myself.  Secondly the "community service" aspect of helping someone who starts figure out the "coordinate system" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade as most other submissions may do.However I think this work is still half-done and even though working on this project is a great idea the authors do not yet do it properly. Firstly I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU ReLU ...) and I do not see how it stands on the same grounds as something as general as "3 Strive for simplicity".Similarly some of the patterns are as vague as "Increase symmetry" and are backed up by statements such as "we noted a special degree of elegance in the FractalNet". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. Some other patterns are phrased with weird names "7 Cover the problem space" - which I guess stands for dataset augmentation; or "6 over-train" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding "overtrain") which then has no connection to the description of "over-train" provided by the authors ("training a network on a harder problem to improve generalization"). If "harder problem" means one where one adds an additional term (i.e. the regularizer) the authors are doing harm to the unexperienced reader confusing "regularization" with something that sounds like "overfitting" (i.e. the exact opposite).Furthermore the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out -how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. -whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes why it is not in the 4.1.2 or 4.1.0) -and most importantly how these design patterns would be deployed in practice to think of a new network. To be more concrete the authors mention that they propose the "freeze-drop-path" variant from "symmetry considerations" to "drop-path". Is this an application of the "increase symmetry" pattern? How would "freeze-drop-path" be more symmetric that "drop-path"? Can this be expressed concretely or is it some intuitive guess? If the second it is not really part of applying a pattern in my understanding. If the first this is missing. What I would have appreciated more (and would like to see in a revised version) would have been a table of "design patterns" on one axis "Deep network" on another and a breakdown of which network applies which design pattern. A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful.   ,Deep Convolutional Neural Network Design Patterns,2.5313,False,['female' None]
This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.Although the paper presents this notion as new basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings the method is not novel either: this method is almost identical to one of the similarity functions presented in "A Simple Word Embedding Model for Lexical Substitution" (Melamud et al. 2015). The novelty claim must be more accurate and position itself with respect to existing work.In addition I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context for example: *,Learning similarity preserving representations with neural similarity and context encoders,0.676,False,['female' None]
There is consensus among the reviewers that the novelty of the paper is limited and that the experimental evaluation of the proposed method needs to be improved.,Learning similarity preserving representations with neural similarity and context encoders,0.296,False,['female' None]
This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.Although the paper presents this notion as new basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings the method is not novel either: this method is almost identical to one of the similarity functions presented in "A Simple Word Embedding Model for Lexical Substitution" (Melamud et al. 2015). The novelty claim must be more accurate and position itself with respect to existing work.In addition I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context for example: * ,Learning similarity preserving representations with neural similarity and context encoders,0.676,False,['female' None]
this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. First considering the related work [12] the proposed approach brings marginal novelty. EspeciallyContext Encoders is just a small improvement over word2vec. Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].[1] ,Learning similarity preserving representations with neural similarity and context encoders,0.9164,False,['female' None]
This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings the architecture and approach is far from novel. As far as I can tell the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course using explicit contextual information is not a new idea especially for tasks like word sense disambiguation (see e.g. 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al which should also be cited) but the specific method used here is original as far as I know.The evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora and as the authors mentioned in the comments they expect "the gain will be less significant as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training."The argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications so it is good to see experiments on NER. However again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.Overall I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.,Learning similarity preserving representations with neural similarity and context encoders,3.4395000000000002,False,['female' None]
The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.I have two main concerns. One is the lack of comparisons to similar recently proposed methods - "Learning Step Size Controllers for Robust Neural Network Training" by Daniel et al. and "Learning to learn by gradient descent by gradient descent" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated but why is this a bad thing?My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly which reinforces the need for comparisons on standard architectures with previously reported results. For example if the baselines used a better architecture like a ResNet or for simplicty Network in Network from this list:,An Actor-critic Algorithm for Learning Rate Learning,-0.3408,False,['male' 'male' 'male' None]
The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other similar approaches and the baselines are suspiciously weak making the proposed method difficult to justify.,An Actor-critic Algorithm for Learning Rate Learning,-0.8519,False,['male' 'male' 'male' None]
The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad Adam RMSProp etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:-What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented even though the success of methods like Adagrad was due to their wall-time performance not the number of iterations.-Why was only a single learning rate learned? To accurately compare against other popular first order methods why not train a separate RL model for each parameter similar to how popular first-order methods adaptively change the learning rate for each parameter.-Since learning is a non-stationary process while RL algorithms assume a stationary environment why should we expect an RL algorithm to work for learning a learning rate?-In figure 6 how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.,An Actor-critic Algorithm for Learning Rate Learning,2.6078,False,['male' 'male' 'male' None]
The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.I have two main concerns. One is the lack of comparisons to similar recently proposed methods - "Learning Step Size Controllers for Robust Neural Network Training" by Daniel et al. and "Learning to learn by gradient descent by gradient descent" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated but why is this a bad thing?My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly which reinforces the need for comparisons on standard architectures with previously reported results. For example if the baselines used a better architecture like a ResNet or for simplicty Network in Network from this list:,An Actor-critic Algorithm for Learning Rate Learning,-0.3408,False,['male' 'male' 'male' None]
In the question response the authors mention and compare other works such as "Learning to Learn by Gradient Descent by Gradient Descent" but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here as if the paper claim the advantage of their method they should use the state of the art network architecture and see if their claim still holds in that setting too.As discussed before the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.In summary the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method. ,An Actor-critic Algorithm for Learning Rate Learning,1.5006,False,['male' 'male' 'male' None]
This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods However my first concern is that from the methodological point of view the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave but the replies were not so convincing. This paper was actually difficult for me to follow. For instance in Figure 1 a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit but what is $w$? I could not find any definition. Furthermore I could not know how $h$ is estimated in this method. Therefore I do NOT understand how this method performs biclustering. Totally I am not sure that this paper is suitable for publication. Prons:Empirical performance is good.Cons:Novelty of the proposed methodSome description in the paper is unclear.,Rectified Factor Networks for Biclustering,-0.41669999999999996,False,[None 'male' 'male']
The reviewers pointed out several issues with the paper and all recommended rejection.,Rectified Factor Networks for Biclustering,-0.4019,False,[None 'male' 'male']
The paper presents a repurposing of rectified factor networks proposedearlier by the same authors to biclustering. The method seemspotentially quite interesting but the paper has serious problems inthe presentation.Quality:The method relies mainly on techniques presented in a NIPS 2015 paperby (mostly) the same authors. The experimental procedure should beclarified further. The results (especially Table 2) seem to dependcritically upon the sparsity of the reported clusters but the authorsdo not explain in sufficient detail how the sparsity hyperparameter isdetermined.Clarity:The style of writing is terrible and completely unacceptable as ascientific publication. The text looks more like an industry whitepaper or advertisement not an objective scientific paper. A completerewrite would be needed before the paper can be considered forpublication. Specifically all references to companies using yourmethods must be deleted.Additionally Table 1 is essentially unreadable. I would recommendusing a figure or cleaning up the table by removing all engineeringnotation and reporting numbers per 1000 so that e.g. "0.475 +/- 9e-4"would become "475 +/- 0.9". In general figures would be preferred as aprimary means for presenting the results in text while tables can beincluded as supplementary information.Originality:The novelty of the work appears limited: the method is mostly based ona NIPS 2015 paper by the same authors. The experimental evaluationappears at least partially novel but for example the IBD detection isvery similar to Hochreiter (2013) but without any comparison.Significance:The authors' strongest claim is based on strong empirical performancein their own benchmark problems. It is however unclear how useful thiswould be to others as there is no code available and the details ofthe implementation are less than complete. Furthermore the methoddepends on many specific tuning parameters whose tuning method is notfully defined leaving it unclear how to guarantee the generalisationof the good performance.,Rectified Factor Networks for Biclustering,1.2107999999999999,False,[None 'male' 'male']
Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l p and m) and I am still not confident that I understand the model being used.Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art FABIA is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)Quality: The experiments are high-quality. Comments:1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis but this claim should be supported with a better argument or experiments.3) How is the model deep? The model isn't deep just because it uses a relu and dropout.,Rectified Factor Networks for Biclustering,1.9123999999999999,False,[None 'male' 'male']
This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods However my first concern is that from the methodological point of view the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave but the replies were not so convincing. This paper was actually difficult for me to follow. For instance in Figure 1 a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit but what is $w$? I could not find any definition. Furthermore I could not know how $h$ is estimated in this method. Therefore I do NOT understand how this method performs biclustering. Totally I am not sure that this paper is suitable for publication. Prons:Empirical performance is good.Cons:Novelty of the proposed methodSome description in the paper is unclear.,Rectified Factor Networks for Biclustering,-0.41669999999999996,False,[None 'male' 'male']
This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.My major criticisms are three-fold: for one the results are not compared to one of the many other pruning methods that are described in section 1.1 and as such the performance of the method is difficult to judge from the paper alone. Second there have been several other compression schemes involving pruning re-training and vector-quantization [e.g. 1 2 3] that seem to achieve much higher accuracies compression ratios and speed-ups. Hence for the practical application of running such networks on low-power low-memory devices other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers in particular the last one. The speed-up in the convolutional layers is comparably small making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.[1] Deep Compression: Compressing Deep Neural Networks with Pruning Trained Quantization and Huffman Coding,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0.22140000000000004,False,['male' 'male' 'male' 'male']
The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques so in the current form the paper is not ready for acceptance.,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,-0.5559,False,['male' 'male' 'male' 'male']
This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive the paper provides good insights.,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0.7273000000000001,False,['male' 'male' 'male' 'male']
I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0.3878999999999999,False,['male' 'male' 'male' 'male']
This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.My major criticisms are three-fold: for one the results are not compared to one of the many other pruning methods that are described in section 1.1 and as such the performance of the method is difficult to judge from the paper alone. Second there have been several other compression schemes involving pruning re-training and vector-quantization [e.g. 1 2 3] that seem to achieve much higher accuracies compression ratios and speed-ups. Hence for the practical application of running such networks on low-power low-memory devices other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers in particular the last one. The speed-up in the convolutional layers is comparably small making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.[1] Deep Compression: Compressing Deep Neural Networks with Pruning Trained Quantization and Huffman Coding ,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0.22140000000000004,False,['male' 'male' 'male' 'male']
I suggest to refer the following two papers.- Kyuyeon Hwang and Wonyong Sung. "Fixed-point feedforward deep neural network design using weights +1 0 and −1." 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE 2014.- Jonghong Kim Kyuyeon Hwang and Wonyong Sung. "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks." 2014 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP). IEEE 2014.The retrain-based neural network quantization algorithm was first published in these two papers.Thanks.,Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network,0.4404,False,['male' 'male' 'male' 'male']
